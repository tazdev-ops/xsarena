# Repo Flat Pack

Instructions for assistant:
- Treat '=== START FILE: path ===' boundaries as file delimiters.
- Do not summarize early; ask for next files if needed.
- Keep references by path for follow-ups.

## Repo Map (selected files)

- README.md  (2441 bytes, sha256:21b27a6dec)
- COMMANDS_REFERENCE.md  (11911 bytes, sha256:0b70005c67)
- pyproject.toml  (1576 bytes, sha256:0d8a4ddb56)
- src/xsarena/cli/main.py  (537 bytes, sha256:891c36b605)
- src/xsarena/cli/registry.py  (8614 bytes, sha256:b637d183ca)
- src/xsarena/cli/context.py  (6677 bytes, sha256:f34df7d057)
- src/xsarena/core/prompt.py  (11946 bytes, sha256:1dc4c5232c)
- src/xsarena/core/prompt_runtime.py  (2049 bytes, sha256:f56964498f)
- src/xsarena/core/v2_orchestrator/orchestrator.py  (19959 bytes, sha256:94b17601e9)
- src/xsarena/core/v2_orchestrator/specs.py  (3005 bytes, sha256:aa7f2bea62)
- src/xsarena/core/jobs/model.py  (17416 bytes, sha256:78549e023e)
- src/xsarena/core/jobs/executor.py  (44 bytes, sha256:624aae4583)
- src/xsarena/core/jobs/scheduler.py  (12098 bytes, sha256:0718434e07)
- src/xsarena/core/jobs/store.py  (4653 bytes, sha256:7eed3d1059)
- src/xsarena/core/config.py  (8037 bytes, sha256:760b70b9ab)
- src/xsarena/core/state.py  (4810 bytes, sha256:d55cba5993)
- src/xsarena/bridge_v2/api_server.py  (8963 bytes, sha256:0e69939d9c)
- directives/_rules/rules.merged.md  (17464 bytes, sha256:a6cdc959a5)
- directives/base/zero2hero.md  (2452 bytes, sha256:81c83b9ca1)
- directives/system/plan_from_seeds.md  (589 bytes, sha256:c0671a0373)

=== START FILE: README.md ===
```markdown
# XSArena

XSArena is a human writer workflow tool that bridges to LMArena for long-form content creation. It focuses on providing a structured approach to writing books, manuals, and other long-form content with AI assistance.

## Quick Start

- **Install**: `pip install -e ".[dev]"`
- **Start bridge**: `xsarena ops service start-bridge-v2`
- **First run**: `xsarena run book "Hello World" --length standard --span medium`

## Essential Workflows

### Book Authoring
Generate comprehensive books from zero to hero level with structured prompts:
```bash
# Create a book with default settings
xsarena run book "Machine Learning Fundamentals"

# Create a longer book with custom length and span
xsarena run book "Advanced Python Programming" --length long --span book

# Plan first, then write
xsarena run from-plan --subject "History of Rome"
```

### Interactive Mode
Start an interactive session for real-time collaboration:
```bash
xsarena interactive start
```

### Study Aids
Generate educational materials from your content:
```bash
# Create flashcards from a text file
xsarena study generate flashcards path/to/content.txt

# Generate a quiz
xsarena study generate quiz path/to/content.txt --num 20

# Create a glossary
xsarena study generate glossary path/to/content.txt
```

### Content Processing
Process and refine content with lossless operations:
```bash
# Rewrite text while preserving meaning
xsarena author lossless-rewrite "Your text here..."

# Improve flow and transitions
xsarena author lossless-improve-flow "Your text here..."

# Enhance structure with headings
xsarena author lossless-enhance-structure "Your text here..."
```

## Command Overview

XSArena is organized into semantic command groups:

- **`run`** - Book generation and long-form content creation
- **`study`** - Educational tools (flashcards, quizzes, glossaries)
- **`author`** - Content creation, ingestion, and style tools
- **`interactive`** - Interactive sessions and real-time collaboration
- **`ops`** - Operations, jobs, settings, and service management
- **`dev`** - Development tools and agent functionality
- **`analyze`** - Content analysis and insights

## Documentation

- [Getting Started](./docs/USAGE.md) - Installation and first steps
- [Workflows](./docs/USAGE.md) - Zero-to-hero workflows and recipes
- [Configuration](./docs/OPERATING_MODEL.md) - Settings and persistence
- [Full docs](./docs/) - Complete documentation directory

```
=== END FILE: README.md ===

=== START FILE: COMMANDS_REFERENCE.md ===
```markdown
# XSArena Command Reference
<!-- This file is the source of truth for CLI usage; regenerate via scripts/gen_docs.sh -->

This document provides a comprehensive reference for all XSArena commands, organized by their semantic groups.

## Command Groups

### Author
Core content creation workflows.

- `xsarena run` - Run a book or recipe in authoring mode (alias for `xsarena author run`)
  - `xsarena run book "Subject"` - Generate a book with specified subject
  - `xsarena run continue <file>` - Continue writing from an existing file
  - `xsarena run from-recipe <file>` - Run a job from a recipe file
  - `xsarena run from-plan` - Plan from rough seeds and run a book
  - `xsarena run template <template> <subject>` - Run a structured directive
  - `xsarena run replay <manifest>` - Replay a job from a run manifest
- `xsarena author interactive` - Start an interactive authoring session

### Interactive Session Commands (REPL)

Commands available within the interactive session (use /command format):

- `/run.inline` - Paste and run a multi-line YAML recipe (end with EOF)
- `/quickpaste` - Paste multiple /commands (end with EOF)
- `/checkpoint.save [name]` - Save current session state to checkpoint
- `/checkpoint.load [name]` - Load session state from checkpoint
- `xsarena author ingest-ack` - Ingest a large document in 'acknowledge' mode with 'OK i/N' handshake loop
- `xsarena author ingest-synth` - Ingest a large document in 'synthesis' mode with rolling update loop
- `xsarena author ingest-style` - Ingest a large document in 'style' mode with rolling style profile update loop
- `xsarena author ingest-run` - Ingest a large document and create a dense synthesis (alias for synth mode)
- `xsarena author lossless-ingest` - Ingest and synthesize information from text
- `xsarena author lossless-rewrite` - Rewrite text while preserving all meaning
- `xsarena author lossless-run` - Perform a comprehensive lossless processing run
- `xsarena author lossless-improve-flow` - Improve the flow and transitions in text
- `xsarena author lossless-break-paragraphs` - Break dense paragraphs into more readable chunks
- `xsarena author lossless-enhance-structure` - Enhance text structure with appropriate headings and formatting
- `xsarena author style-narrative` - Enable or disable the narrative/pedagogy overlay for the session
- `xsarena author style-nobs` - Enable or disable the no-bullshit (no-bs) language overlay
- `xsarena author style-reading` - Enable or disable the further reading overlay for the session
- `xsarena author style-show` - Show currently active overlays
- `xsarena author style-apply` - Generate content on a new subject using a captured style profile file
- `xsarena author workshop` - Workshop tools
- `xsarena author preview` - Preview tools
- `xsarena author post-process` - Post-processing tools (aliases to utils tools)
  - `xsarena author post-process export-chapters <book>` - Export a book into chapters with navigation links (alias to xsarena utils tools export-chapters)
  - `xsarena author post-process extract-checklists --book <book>` - Extract checklist items from a book (alias to xsarena utils tools extract-checklists)

### Analyze
Analysis and evidence-based tools.

- `xsarena analyze coverage --outline <file> --book <file>` - Analyze coverage of a book against an outline
- `xsarena analyze continuity` - Analyze book continuity for anchor drift and re-introductions
- `xsarena analyze style-lint <path>` - Lint directive files for best practices
- `xsarena analyze secrets [path]` - Scan for secrets (API keys, passwords, etc.)
- `xsarena analyze chad` - CHAD analysis tools

### Study
Study aids, learning tools, and practice drills.

- `xsarena study generate` - Generate study materials
  - `xsarena study generate flashcards <content_file>` - Generate flashcards from a content file
  - `xsarena study generate quiz <content_file>` - Generate a quiz from a content file
  - `xsarena study generate glossary <content_file>` - Create a glossary from a content file with frequency filtering
  - `xsarena study generate index <content_file>` - Generate an index from a content file with depth control
  - `xsarena study generate cloze <content_file>` - Create cloze deletions from a content file
  - `xsarena study generate drill <content_file>` - Generate active recall drills from a content file
- `xsarena study coach` - Coaching tools
- `xsarena study joy` - Joy-related tools (hidden)

### Dev
Coding agent, git integration, automation pipelines, and simulation.

- `xsarena dev agent` - Coding agent tools
- `xsarena dev pipeline` - Pipeline management
- `xsarena dev simulate <subject>` - Run a fast offline simulation

### Project
Project management and initialization.

- `xsarena project project` - Project-related commands
- `xsarena project init` - Initialize a new project

### Ops
System health, jobs, services, and configuration.

- `xsarena ops service` - Service management
- `xsarena ops jobs` - Job management
- `xsarena ops health` - System health, maintenance, and self-healing operations
- `xsarena ops handoff` - Prepare higher-AI handoffs
  - `xsarena ops handoff prepare` - Build snapshot and brief for higher AI handoff
  - `xsarena ops handoff note` - Add notes to the latest handoff request
  - `xsarena ops handoff show` - Show the latest handoff package details
- `xsarena ops orders` - Manage ONE ORDER log
  - `xsarena ops orders new` - Create a new order with title and body
  - `xsarena ops orders ls` - List recent orders
  - `xsarena ops health fix-run` - Self-heal common configuration/state issues
  - `xsarena ops health sweep` - Purge ephemeral artifacts by TTL
  - `xsarena ops health scan-secrets` - Scan for secrets (API keys, passwords, etc.) in working tree
  - `xsarena ops health mark` - Add an XSA-EPHEMERAL header to a helper script so the sweeper can purge it later
  - `xsarena ops health read` - Read startup plan; attempt merge; print sources found
  - `xsarena ops health init` - One-time helper: create a minimal rules baseline if merged rules and sources are missing
- `xsarena ops snapshot` - Snapshot management
  - `xsarena ops snapshot create` - Create a flat snapshot, ideal for chatbot uploads (recommended)
    - `xsarena ops snapshot create --mode ultra-tight --total-max 2500000 --max-per-file 180000` - Ultra-tight preset (recommended)
    - `xsarena ops snapshot create --mode author-core --total-max 4000000 --max-per-file 200000` - Author core preset (alternative)
    - `xsarena ops snapshot create --mode custom -I README.md -I src/xsarena/core/prompt.py --out repo_flat.txt` - Custom includes
  - `xsarena ops snapshot debug-report` - Generate a verbose snapshot for debugging (formerly 'pro')
  - `xsarena ops snapshot verify` - Verify snapshot health: preflight or postflight
- `xsarena ops debug` - Debugging commands
- `xsarena ops directives` - Directive tools (index)
- `xsarena ops booster` - Interactively engineer and improve prompts
- `xsarena ops adapt` - Adaptive inspection and safe fixes
  - `xsarena ops adapt inspect` - Analyze repo state and write a plan (no changes)
  - `xsarena ops adapt fix` - Apply safe, targeted fixes (no refactors)
  - `xsarena ops adapt plan` - Alias to inspect (compat)
  - `xsarena ops adapt suppress-add` - Add suppression patterns to avoid false positives
  - `xsarena ops adapt suppress-ls` - List current suppression patterns
  - `xsarena ops adapt suppress-clear` - Clear suppression patterns

### Top-Level Commands
Essential commands available at the top level.

- `xsarena run` - Run a book or recipe in authoring mode (alias for `xsarena author run`)
- `xsarena interactive` - Interactive authoring session (alias for `xsarena author interactive`)
- `xsarena settings` - Unified settings interface (configuration + controls)
- `xsarena report` - Create diagnostic reports
  - `xsarena report quick` - Generate quick diagnostic report
  - `xsarena report job` - Generate detailed job-specific report
  - `xsarena report full` - Generate full debug report with pro snapshot

### Deprecated Commands

- `xsarena ops doctor` - System health checks (DEPRECATED → use xsarena ops health ...)

## Settings Commands

The `xsarena settings` group provides unified access to both configuration and controls settings:

- `xsarena settings show` - Show both configuration and controls settings
- `xsarena settings set` - Set configuration or controls settings with various options:
  - `--backend` - Set backend (ops settings)
  - `--model` - Set default model (ops settings)
  - `--base-url` - Set base URL for bridge backend (ops settings)
  - `--api-key` - Set API key (ops settings)
  - `--output-min-chars` - Set minimal chars per chunk (utils settings)
  - `--output-push-max-passes` - Set max extension steps per chunk (utils settings)
  - `--continuation-mode` - Set continuation mode (utils settings)
  - `--anchor-length-config` - Set config anchor length (ops settings)
  - `--anchor-length-control` - Set control anchor length (utils settings)
  - `--repetition-threshold` - Set repetition detection threshold (utils settings)
  - `--repetition-warn/--no-repetition-warn` - Enable or disable repetition warning (utils settings)
  - `--coverage-hammer/--no-coverage-hammer` - Enable or disable coverage hammer (utils settings)
  - `--output-budget/--no-output-budget` - Enable or disable output budget addendum (utils settings)
  - `--output-push/--no-output-push` - Enable or disable output pushing (utils settings)
- `xsarena settings persist` - Persist current CLI knobs to .xsarena/config.yml (controls layer) and save config (config layer)
- `xsarena settings reset` - Reset settings from persisted configuration (controls layer) and reload config (config layer)

## Jobs Commands

The `xsarena ops jobs` group provides job management:

- `xsarena ops jobs list` - List all jobs
- `xsarena ops jobs show <job_id>` - Show details of a specific job
- `xsarena ops jobs follow <job_id>` - Follow a job to completion
- `xsarena ops jobs cancel <job_id>` - Cancel a running job
- `xsarena ops jobs pause <job_id>` - Pause a running job
- `xsarena ops jobs resume <job_id>` - Resume a paused job
- `xsarena ops jobs next <job_id> <hint>` - Send a hint to the next chunk of a job
- `xsarena ops jobs clone <job_id>` - Clone a job directory into a new job with a fresh id

## Run Commands

The `xsarena run` group provides various ways to run content generation:

- `xsarena run book <subject>` - Generate a book with specified subject
  - `--profile <profile>` - Use a specific profile
  - `--length <length>` - Set length preset (standard|long|very-long|max)
  - `--span <span>` - Set span preset (medium|long|book)
  - `--extra-file <file>` - Append file(s) to system prompt
  - `--out <path>` - Set output path
  - `--wait` - Wait for browser capture before starting
  - `--plan` - Generate an outline first
  - `--follow` - Submit job and follow to completion
- `xsarena author run continue <file>` - Continue writing from an existing file
- `xsarena author run from-recipe <file>` - Run a job from a recipe file
- `xsarena author run from-plan` - Plan from rough seeds and run a book
- `xsarena author run template <template> <subject>` - Run a structured directive from the library
- `xsarena author run replay <manifest>` - Replay a job from a run manifest

## Tools Commands

Various utility commands are available through the utils group:

- `xsarena utils tools eli5 <topic>` - Explain like I'm five
- `xsarena utils tools story <concept>` - Explain the concept with a short story
- `xsarena utils tools persona <name>` - Set persona overlay (chad|prof|coach)
- `xsarena utils tools nobs <on|off>` - Toggle no-BS setting
- `xsarena utils tools export-chapters <book>` - Export a book into chapters with navigation links
- `xsarena utils tools extract-checklists --book <book>` - Extract checklist items from a book

```
=== END FILE: COMMANDS_REFERENCE.md ===

=== START FILE: pyproject.toml ===
```toml
[build-system]
requires = ["setuptools>=45", "wheel", "setuptools_scm[toml]>=6.2"]
build-backend = "setuptools.build_meta"

[project]
name = "xsarena"
version = "0.2.0"
description = "AI-powered writing and coding studio"
readme = "README.md"
requires-python = ">=3.9"
license = {text = "MIT"}
authors = [
    {name = "XSArena Team"}
]
dependencies = [
    "typer>=0.9.0",
    "python-dotenv>=1.0.0",
    "aiohttp>=3.8.0",
    "pydantic>=2.0.0",
    "PyYAML>=6.0",
    "rich>=13.0.0",
    "fastapi>=0.100.0",
    "uvicorn[standard]>=0.23.0",
    "websockets>=11.0",
    "requests>=2.31.0",
    "jsonschema>=4.18.0"
]

[project.optional-dependencies]

dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "mypy>=1.0.0",
    "ruff>=0.1.0",
    "black>=23.0.0"
]
watch = [
    "watchdog>=3.0.0"
]
metrics = [
    "prometheus-client>=0.19.0"
]


[project.scripts]
xsarena = "xsarena.cli.main:run"
# xsarena-tui = "xsarena_tui:main"   # moved to contrib/; no longer maintained as core feature
xsarena-bridge = "xsarena.bridge_v2.api_server:run_server"
# compatibility for one release cycle:
lmastudio = "xsarena.cli.main:run"
lmastudio-bridge = "xsarena.bridge_v2.api_server:run_server"

[project.urls]
Homepage = "[REDACTED_URL]"
Repository = "[REDACTED_URL]"

[tool.setuptools.packages.find]
where = ["src"]

[tool.ruff]
line-length = 100

[tool.ruff.lint]
select = ["E", "W", "F", "I", "C", "B", "SIM"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
asyncio_mode = "auto"

```
=== END FILE: pyproject.toml ===

=== START FILE: src/xsarena/cli/main.py ===
```python
# Main CLI entry point for xsarena
"""This module provides the main command-line interface for xsarena."""
import typer

from .registry import app


@app.command("version")
def _version():
    from .. import __version__

    typer.echo(f"XSArena v{__version__}")


def run():
    """Run the CLI application."""
    # Simple logging setup
    import logging

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )
    app()


if __name__ == "__main__":
    run()

```
=== END FILE: src/xsarena/cli/main.py ===

=== START FILE: src/xsarena/cli/registry.py ===
```python
# src/xsarena/cli/registry.py
"""CLI command registry for XSArena."""

import typer

from ..core.config import Config

# Import all command modules
from .cmds_agent import app as agent_app
from .cmds_analyze import app as analyze_app
from .cmds_audio import app as audio_app
from .cmds_bilingual import app as bilingual_app
from .cmds_booster import app as booster_app
try:
    from .cmds_chad import app as chad_app
except ImportError:
    # chad module is optional
    chad_app = None
from .cmds_checklist import app as checklist_app
from .cmds_coach import app as coach_app
from .cmds_coder import app as coder_app
from .cmds_controls import app as controls_app
from .cmds_debug import app as debug_app
from .cmds_dev import app as dev_app
from .cmds_directives import app as directives_app
from .cmds_docs import app as docs_app
from .cmds_endpoints import app as endpoints_app
from .cmds_joy import app as joy_app
from .cmds_json import app as json_app
from .cmds_list import app as list_app
from .cmds_macros import app as macros_app
from .cmds_metrics import app as metrics_app
from .cmds_modes import app as modes_app
from .cmds_people import app as people_app
from .cmds_pipeline import app as pipeline_app
from .cmds_playground import app as playground_app
from .cmds_policy import app as policy_app
from .cmds_preview import app as preview_app
from .cmds_project import app as project_app
from .cmds_publish import app as publish_app
from .cmds_study import app as study_app
from .cmds_upgrade import app as upgrade_app
from .cmds_workshop import app as workshop_app
from .cmds_health import app as health_app
from .cmds_interactive import app as interactive_app
from .cmds_jobs import app as jobs_app
from .cmds_report import app as report_app
from .cmds_run import app as run_app
from .cmds_snapshot import app as snapshot_app
from .cmds_tools import app as tools_app
from .cmds_unified_settings import app as unified_settings_app

# Import roles and overlays commands
from .cmds_directives import roles_list, roles_show, overlays_list, overlays_show

# --- Global CLI context init (ensures ctx.obj is set for all commands)
from .context import CLIContext
from .service import app as service_app

# --- Main App ---
app = typer.Typer(help="XSArena — AI-powered writing and coding studio")


# --- Global CLI context init (ensures ctx.obj is set for all commands)
@app.callback()
def _init_ctx(
    ctx: typer.Context,
    backend: str = typer.Option(
        None, "--backend", help="Override backend for this invocation"
    ),
    model: str = typer.Option(
        None, "--model", help="Override model for this invocation"
    ),
    base_url: str = typer.Option(None, "--base-url", help="Override bridge base URL"),
):
    cfg_over = None
    if any([backend, model, base_url]):
        cfg_over = Config(
            backend=backend or "bridge",
            model=model or "default",
            base_url=base_url or "[REDACTED_URL]",
        )
    ctx.obj = CLIContext.load(cfg=cfg_over)


# --- Essential Top-Level Commands ---
app.add_typer(run_app, name="run", help="Run a book or recipe in authoring mode")
app.add_typer(interactive_app, name="interactive", help="Interactive authoring session")
app.add_typer(
    unified_settings_app,
    name="settings",
    help="Unified settings interface (configuration + controls)",
)

# --- Semantic Command Groups ---
author_app = typer.Typer(name="author", help="Core content creation workflows.")

# Add post-process tools as a subgroup under author
from .cmds_tools import export_chapters_cmd, extract_checklists_cmd

post_process_app = typer.Typer(
    name="post-process", help="Post-processing tools (aliases to utils tools)"
)
post_process_app.command("export-chapters")(export_chapters_cmd)
post_process_app.command("extract-checklists")(extract_checklists_cmd)
author_app.add_typer(post_process_app, name="post-process")

app.add_typer(author_app)

# Add authoring commands directly to the author app
from .cmds_authoring import (
    ingest_ack,
    ingest_run,
    ingest_style,
    ingest_synth,
    lossless_break_paragraphs,
    lossless_enhance_structure,
    lossless_improve_flow,
    lossless_ingest,
    lossless_rewrite,
    lossless_run,
    style_narrative,
    style_nobs,
    style_reading,
    style_show,
)

author_app.command("ingest-ack")(ingest_ack)
author_app.command("ingest-synth")(ingest_synth)
author_app.command("ingest-style")(ingest_style)
author_app.command("ingest-run")(ingest_run)
author_app.command("lossless-ingest")(lossless_ingest)
author_app.command("lossless-rewrite")(lossless_rewrite)
author_app.command("lossless-run")(lossless_run)
author_app.command("lossless-improve-flow")(lossless_improve_flow)
author_app.command("lossless-break-paragraphs")(lossless_break_paragraphs)
author_app.command("lossless-enhance-structure")(lossless_enhance_structure)
author_app.command("style-narrative")(style_narrative)
author_app.command("style-nobs")(style_nobs)
author_app.command("style-reading")(style_reading)
author_app.command("style-show")(style_show)

from .cmds_settings import app as config_app

ops_app = typer.Typer(
    name="ops", help="System health, jobs, services, and configuration."
)
ops_app.add_typer(service_app, name="service")
ops_app.add_typer(jobs_app, name="jobs")
ops_app.add_typer(
    health_app,
    name="health",
    help="System health, maintenance, and self-healing operations",
)
ops_app.add_typer(snapshot_app, name="snapshot")
ops_app.add_typer(
    config_app, name="config", help="Configuration and backend management"
)
# Import and register the new command groups
from .cmds_handoff import app as handoff_app
from .cmds_orders import app as orders_app

app.add_typer(report_app, name="report", help="Create diagnostic reports")
ops_app.add_typer(handoff_app, name="handoff", help="Prepare higher-AI handoffs")
ops_app.add_typer(orders_app, name="orders", help="Manage ONE ORDER log")

app.add_typer(ops_app)

# --- Additional Semantic Groups ---
utilities_group = typer.Typer(name="utils", help="General utility commands.")

utilities_group.add_typer(
    tools_app,
    name="tools",
    help="Utility tools like chapter export and checklist extraction.",
)

app.add_typer(utilities_group)

# Documentation group
app.add_typer(docs_app, name="docs", help="Documentation generation commands")

# Directives group
app.add_typer(directives_app, name="directives", help="Directive utilities (roles, overlays, etc.)")

# Create sub-apps for roles and overlays to match test expectations
roles_app = typer.Typer(name="roles", help="Manage roles")
roles_app.command("list")(roles_list)
roles_app.command("show")(roles_show)
app.add_typer(roles_app)

overlays_app = typer.Typer(name="overlays", help="Manage overlays") 
overlays_app.command("list")(overlays_list)
overlays_app.command("show")(overlays_show)
app.add_typer(overlays_app)

# Add the missing command groups
dev_app = typer.Typer(name="dev", help="Development tools and agent functionality.")
dev_app.add_typer(agent_app, name="agent")
app.add_typer(dev_app)

app.add_typer(analyze_app, name="analyze")
app.add_typer(audio_app, name="audio", hidden=True)  # Hidden as per changelog
app.add_typer(bilingual_app, name="bilingual")
app.add_typer(booster_app, name="booster", help="Interactively engineer and improve prompts")
if chad_app:
    app.add_typer(chad_app, name="chad")
app.add_typer(checklist_app, name="checklist")
app.add_typer(coach_app, name="coach")
app.add_typer(coder_app, name="coder")
app.add_typer(controls_app, name="controls", help="Fine-tune output, continuation, and repetition behavior.")
ops_app.add_typer(debug_app, name="debug", help="Debugging commands")  # Add to ops
app.add_typer(directives_app, name="directives")
app.add_typer(endpoints_app, name="endpoints")
app.add_typer(joy_app, name="joy", hidden=True)  # Hidden
app.add_typer(json_app, name="json")
app.add_typer(list_app, name="list")
app.add_typer(macros_app, name="macros")
app.add_typer(metrics_app, name="metrics")
app.add_typer(modes_app, name="modes", hidden=True)
app.add_typer(people_app, name="people", hidden=True)  # Hidden
app.add_typer(pipeline_app, name="pipeline")
app.add_typer(playground_app, name="playground")
app.add_typer(policy_app, name="policy", hidden=True)  # Hidden
app.add_typer(preview_app, name="preview")
app.add_typer(project_app, name="project")
app.add_typer(publish_app, name="publish")
app.add_typer(study_app, name="study")
app.add_typer(upgrade_app, name="upgrade")
app.add_typer(workshop_app, name="workshop", hidden=True)  # Hidden

if __name__ == "__main__":
    app()

```
=== END FILE: src/xsarena/cli/registry.py ===

=== START FILE: src/xsarena/cli/context.py ===
```python
from __future__ import annotations

import contextlib
import os
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

from ..core.backends import create_backend
from ..core.config import Config
from ..core.engine import Engine
from ..core.redact import redact
from ..core.state import SessionState


@dataclass
class CLIContext:
    config: Config
    state: SessionState
    engine: Engine
    state_path: Path

    @classmethod
    def load(  # noqa: C901
        cls, cfg: Optional[Config] = None, state_path: Optional[str] = None
    ) -> "CLIContext":
        """
        Load CLI context with clear order of precedence:
        1. Start with hardcoded SessionState() defaults
        2. Load .xsarena/config.yml (project-level defaults)
        3. Load .xsarena/session_state.json (user's last-used
           interactive settings) - OVERRIDES config
        4. Apply CLI flags from cfg object (explicit, one-time
           overrides) - HIGHEST priority
        """
        # Start with hardcoded defaults
        session_state = SessionState()

        # Set up state path
        state_path = Path(state_path or "./.xsarena/session_state.json")
        state_path.parent.mkdir(parents=True, exist_ok=True)

        # 2. Load .xsarena/config.yml (project-level defaults) FIRST
        config_path = Path(".xsarena/config.yml")
        if config_path.exists():
            import yaml

            try:
                with open(config_path, "r", encoding="utf-8") as f:
                    config_content = yaml.safe_load(f) or {}
                persisted_settings = config_content.get("settings", {})

                # Apply config.yml settings, overriding defaults
                for key, value in persisted_settings.items():
                    if hasattr(session_state, key):
                        setattr(session_state, key, value)
            except Exception:
                pass  # If config can't be read, continue with defaults

        # 3. Load .xsarena/session_state.json (user's last-used settings) SECOND - OVERRIDES config
        if state_path.exists():
            try:
                file_session_state = SessionState.load_from_file(str(state_path))
                # Apply ALL session_state values, OVERRIDING config.yml settings
                for field_name in file_session_state.__dict__:
                    if hasattr(session_state, field_name):
                        setattr(
                            session_state,
                            field_name,
                            getattr(file_session_state, field_name),
                        )
            except Exception:
                # Create backup and continue with current state if corrupted
                bak = state_path.with_suffix(f".{int(time.time())}.bak")
                with contextlib.suppress(Exception):
                    state_path.rename(bak)
                # Continue with current state if session file is corrupted

        # Load base configuration for backend settings (config.yml is single source of truth)
        base_cfg = Config.load_from_file(".xsarena/config.yml")

        # 4. Apply CLI flags from cfg object (explicit, one-time overrides)
        # This is the highest priority - CLI flags override everything else including config.yml
        if cfg is not None:
            # Only apply non-default values from CLI to avoid overriding user choices with defaults
            if cfg.backend != "bridge":
                session_state.backend = cfg.backend
            if cfg.model != "default":
                session_state.model = cfg.model
            if cfg.window_size != 100:
                session_state.window_size = cfg.window_size
            if cfg.continuation_mode != "anchor":
                session_state.continuation_mode = cfg.continuation_mode
            if cfg.anchor_length != 300:
                session_state.anchor_length = cfg.anchor_length
            if cfg.repetition_threshold != 0.35:
                session_state.repetition_threshold = cfg.repetition_threshold

        # Normalize base URL shape
        final_base_url = base_cfg.base_url
        if final_base_url and not final_base_url.rstrip("/").endswith("/v1"):
            base_cfg.base_url = final_base_url.rstrip("/") + "/v1"

        # Build engine using the final state
        backend = create_backend(
            session_state.backend,
            base_url=os.getenv("XSA_BRIDGE_URL", base_cfg.base_url),
            api_key=base_cfg.api_key,
            model=session_state.model,
        )
        eng = Engine(backend, session_state)
        if session_state.settings.get("redaction_enabled"):
            eng.set_redaction_filter(redact)
        return cls(
            config=base_cfg, state=session_state, engine=eng, state_path=state_path
        )

    def rebuild_engine(self):
        # self-heal base_url shape
        if self.config.base_url and not self.config.base_url.rstrip("/").endswith(
            "/v1"
        ):
            self.config.base_url = self.config.base_url.rstrip("/") + "/v1"

        self.engine = Engine(
            create_backend(
                self.state.backend,
                base_url=os.getenv("XSA_BRIDGE_URL", self.config.base_url),
                api_key=self.config.api_key,
                model=self.state.model,
            ),
            self.state,
        )

        # Reapply redaction filter if enabled
        if self.state.settings.get("redaction_enabled"):
            self.engine.set_redaction_filter(redact)

    def save(self):
        self.state.save_to_file(str(self.state_path))

    def fix(self) -> list[str]:
        """Attempt self-fixes: base_url shape, backend validity, engine rebuild."""
        notes: list[str] = []
        # normalize base_url
        if self.config.base_url and not self.config.base_url.rstrip("/").endswith(
            "/v1"
        ):
            self.config.base_url = self.config.base_url.rstrip("/") + "/v1"
            notes.append("Normalized base_url to end with /v1")

        # fallback backend if invalid
        if self.state.backend not in ("bridge", "openrouter", "lmarena", "lmarena-ws"):
            self.state.backend = "bridge"
            notes.append("Backend invalid; set to bridge")

        # default base_url if empty
        if not self.config.base_url:
            self.config.base_url = "[REDACTED_URL]"
            notes.append("Set default bridge base_url [REDACTED_URL]")

        self.rebuild_engine()
        self.save()
        if not notes:
            notes.append("No changes; config/state already consistent")
        return notes

```
=== END FILE: src/xsarena/cli/context.py ===

=== START FILE: src/xsarena/core/prompt.py ===
```python
"""Prompt Composition Layer (PCL) - Centralized prompt composition with schema and lints."""

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

from ..utils.project_paths import get_project_root


def _directives_root() -> Path:
    env = os.getenv("XSARENA_DIRECTIVES_ROOT")
    if env:
        p = Path(env)
        if p.exists():
            return p
    # Use robust project root resolution
    return get_project_root() / "directives"


@dataclass
class PromptComposition:
    """Result of prompt composition with applied settings and warnings."""

    system_text: str
    applied: Dict[str, Any]
    warnings: List[str]


class PromptCompositionLayer:
    """Centralized prompt composition with schema and lints."""

    # Base modes and their descriptions
    BASE_MODES = {
        "zero2hero": "pedagogical manual from foundations to practice",
        "reference": "tight reference handbook with definitions first",
        "pop": "accessible narrative explainer with vignettes",
        "nobs": "no‑bullshit manual with tight prose",
    }

    # Overlay options
    OVERLAYS = {
        "no_bs": "Plain language. No fluff. Concrete nouns; tight sentences.",
        "narrative": "Teach-before-use narrative. Define terms at first mention.",
        "compressed": "Compressed narrative. Minimal headings; dense flow.",
        "bilingual": "Mirror structure and translate line-for-line as pairs.",
    }

    def __init__(self):
        self._load_extended_templates()

    def _load_extended_templates(self):
        """Load richer templates from directive files directly."""

        # Load narrative overlay from directive
        narrative_path = _directives_root() / "style" / "narrative.md"
        if narrative_path.exists():
            try:
                with open(narrative_path, "r", encoding="utf-8") as f:
                    narrative_content = f.read().strip()
                    if narrative_content:
                        self.OVERLAYS["narrative"] = narrative_content
            except Exception:
                # Fallback to description if file reading fails
                self.OVERLAYS[
                    "narrative"
                ] = "Teach-before-use narrative. Define terms at first mention."
        else:
            # Fallback to description if file doesn't exist
            self.OVERLAYS[
                "narrative"
            ] = "Teach-before-use narrative. Define terms at first mention."

        # Load compressed overlay from directive
        compressed_path = _directives_root() / "style" / "compressed.md"
        if compressed_path.exists():
            try:
                with open(compressed_path, "r", encoding="utf-8") as f:
                    compressed_content = f.read().strip()
                    if compressed_content:
                        self.OVERLAYS["compressed"] = compressed_content
            except Exception:
                # Fallback to description if file reading fails
                self.OVERLAYS[
                    "compressed"
                ] = "Compressed narrative. Minimal headings; dense flow."
        else:
            # Fallback to description if file doesn't exist
            self.OVERLAYS[
                "compressed"
            ] = "Compressed narrative. Minimal headings; dense flow."

        # Load no_bs overlay from directive
        no_bs_path = _directives_root() / "style" / "no_bs.md"
        if no_bs_path.exists():
            try:
                with open(no_bs_path, "r", encoding="utf-8") as f:
                    no_bs_content = f.read().strip()
                    if no_bs_content:
                        self.OVERLAYS["no_bs"] = no_bs_content
            except Exception:
                # Fallback to description if file reading fails
                self.OVERLAYS[
                    "no_bs"
                ] = "Plain language. No fluff. Concrete nouns; tight sentences."
        else:
            # Fallback to description if file doesn't exist
            self.OVERLAYS[
                "no_bs"
            ] = "Plain language. No fluff. Concrete nouns; tight sentences."

    def compose(
        self,
        subject: str,
        base: str = "zero2hero",
        overlays: Optional[List[str]] = None,
        extra_notes: Optional[str] = None,
        min_chars: int = 4200,
        passes: int = 1,
        max_chunks: int = 12,
    ) -> PromptComposition:
        """
        Compose a final system prompt from base mode, overlays, and subject.

        Args:
            subject: The subject to write about
            base: Base mode (zero2hero, reference, pop, nobs)
            overlays: List of style overlays to apply
            extra_notes: Additional domain-specific notes
            min_chars: Minimum chars per chunk
            passes: Auto-extend passes per chunk
            max_chunks: Maximum number of chunks

        Returns:
            PromptComposition with system_text, applied settings, and warnings
        """
        if overlays is None:
            overlays = []

        # Validate inputs
        warnings = []
        if base not in self.BASE_MODES:
            base = "zero2hero"  # fallback
            warnings.append(f"Unknown base mode '{base}', using 'zero2hero'")

        invalid_overlays = [ov for ov in overlays if ov not in self.OVERLAYS]
        if invalid_overlays:
            warnings.extend([f"Unknown overlay '{ov}'" for ov in invalid_overlays])
            overlays = [ov for ov in overlays if ov in self.OVERLAYS]

        # Build the system text
        parts = []

        # Base intent
        if base == "zero2hero":
            # Load the zero2hero template from directive file

            zero2hero_path = _directives_root() / "base" / "zero2hero.md"

            if zero2hero_path.exists():
                try:
                    with open(zero2hero_path, "r", encoding="utf-8") as f:
                        zero2hero_content = f.read().strip()
                        # Replace {subject} placeholder with actual subject
                        zero2hero_content = zero2hero_content.replace(
                            "{subject}", subject
                        )
                        parts.append(zero2hero_content)
                except Exception:
                    # Fallback to hardcoded content if file reading fails
                    parts.append(
                        "Goal: pedagogical manual from foundations to practice with steady depth; no early wrap-ups."
                    )
            else:
                # Fallback to hardcoded content if file doesn't exist
                parts.append(
                    "Goal: pedagogical manual from foundations to practice with steady depth; no early wrap-ups."
                )
        elif base == "reference":
            parts.append(
                "Goal: tight reference handbook; definitions first; terse, unambiguous rules and examples."
            )
        elif base == "pop":
            parts.append(
                "Goal: accessible, accurate narrative explainer with vignettes; keep rigor without academic padding."
            )
        elif base == "nobs":
            parts.append(
                "Goal: no‑bullshit manual; only what changes decisions or understanding; tight prose."
            )

        # Apply overlays
        for overlay_key in overlays:
            overlay_text = self.OVERLAYS.get(overlay_key, "")
            if overlay_text:
                parts.append(overlay_text)

        # Add continuation rules
        parts.append(
            "Continuation: continue exactly from anchor; do not restart sections; do not summarize prematurely. "
            "If nearing length limit, stop cleanly with: NEXT: [Continue]."
        )

        # Add extra notes if provided
        if extra_notes and extra_notes.strip():
            parts.append(extra_notes.strip())

        # Add domain-specific notes
        if "law" in subject.lower() or "policy" in subject.lower():
            parts.append("This is educational, not legal advice.")

        system_text = "\n".join(parts)

        # Track what was applied
        applied = {
            "subject": subject,
            "base": base,
            "overlays": overlays,
            "extra_notes": extra_notes,
            "continuation": {
                "mode": "anchor",
                "min_chars": min_chars,
                "passes": passes,
                "max_chunks": max_chunks,
                "repeat_warn": True,
            },
        }

        return PromptComposition(
            system_text=system_text, applied=applied, warnings=warnings
        )

    def lint(self, subject: str, base: str, overlays: List[str]) -> List[str]:
        """Perform basic linting on prompt composition parameters."""
        warnings = []

        # Check for subject length
        if len(subject.strip()) < 3:
            warnings.append(
                "Subject is very short (< 3 chars), consider being more specific"
            )

        # Check for common overlay conflicts
        if "narrative" in overlays and "compressed" in overlays:
            warnings.append(
                "Using both 'narrative' and 'compressed' overlays may create conflicting styles"
            )

        # Check base mode appropriateness for subject
        if "law" in subject.lower() and base == "pop":
            warnings.append(
                "For legal subjects, 'reference' or 'nobs' base modes might be more appropriate than 'pop'"
            )

        return warnings


# Global instance for convenience
pcl = PromptCompositionLayer()


def compose_prompt(
    subject: str,
    base: str = "zero2hero",
    overlays: Optional[List[str]] = None,
    extra_notes: Optional[str] = None,
    min_chars: int = 4200,
    passes: int = 1,
    max_chunks: int = 12,
    use_cache: bool = False,  # Caching disabled - parameter kept for backward compatibility
    outline_first: bool = False,  # New parameter for outline-first functionality
    apply_reading_overlay: bool = False,  # New parameter to control reading overlay
) -> PromptComposition:
    """Convenience function to compose a prompt using the global PCL instance."""
    # Caching is disabled due to missing module; always call pcl.compose directly
    result = pcl.compose(
        subject=subject,
        base=base,
        overlays=overlays,
        extra_notes=extra_notes,
        min_chars=min_chars,
        passes=passes,
        max_chunks=max_chunks,
    )

    # If outline_first is enabled, modify the system text to include outline-first instructions
    if outline_first:
        outline_instruction = (
            "\n\nOUTLINE-FIRST SCAFFOLD\n"
            "- First chunk: produce a chapter-by-chapter outline consistent with the subject; end with NEXT: [Begin Chapter 1].\n"
            "- Subsequent chunks: follow the outline; narrative prose; define terms once; no bullet walls."
        )
        result.system_text += outline_instruction
        # Update applied metadata to reflect the outline-first mode
        if "outline_first" not in result.applied:
            result.applied["outline_first"] = True

    # If reading overlay is enabled, add the reading overlay instruction
    if apply_reading_overlay:
        reading_instruction = (
            "\n\nDOMAIN-AWARE FURTHER READING\n"
            "- At the end of major sections, include a 'Further Reading' box with 2-3 curated references.\n"
            "- Use domain-specific resources from data/resource_map.en.json if available.\n"
            "- Format: 'Further Reading: [Resource 1]; [Resource 2]; [Resource 3]'\n"
        )
        result.system_text += reading_instruction
        # Update applied metadata to reflect the reading overlay
        result.applied["reading_overlay"] = True

    return result

```
=== END FILE: src/xsarena/core/prompt.py ===

=== START FILE: src/xsarena/core/prompt_runtime.py ===
```python
"""Runtime utilities for prompt construction and management."""

from typing import Optional


def build_chunk_prompt(
    chunk_idx: int,
    job: "JobV3",
    session_state: Optional["SessionState"] = None,
    next_hint: Optional[str] = None,
    anchor: Optional[str] = None,
) -> str:
    """
    Build the user prompt for a specific chunk based on the current state and context.

    Args:
        chunk_idx: Current chunk index (1-based)
        job: The current job object
        session_state: Optional session state with configuration
        next_hint: Optional hint from previous chunks
        anchor: Optional text anchor for continuation

    Returns:
        The constructed user prompt string
    """
    from ..anchor_service import build_anchor_continue_prompt

    # For the first chunk, use a "BEGIN" style seed
    if chunk_idx == 1:
        hint_now = next_hint
        user_content = hint_now or "BEGIN"
        if hint_now:
            # Log hint application if needed
            pass  # Caller should handle logging

        # Apply outline-first toggle for the first chunk only if enabled
        if session_state and getattr(session_state, "outline_first_enabled", False):
            user_content = "BEGIN\nOUTLINE-FIRST SCAFFOLD\n- First chunk: produce a chapter-by-chapter outline consistent with the subject; end with NEXT: [Begin Chapter 1].\n- Subsequent chunks: follow the outline; narrative prose; define terms once; no bullet walls."
    else:
        # For subsequent chunks, implement anchored continuation
        user_content = build_anchor_continue_prompt(anchor) if anchor else ""

        # Override with next_hint if available
        if next_hint:
            user_content = next_hint

    # Add coverage hammer text if enabled in session state (after first chunk)
    if (
        chunk_idx > 1
        and session_state
        and getattr(session_state, "coverage_hammer_on", False)
    ):
        user_content += "\nCOVERAGE HAMMER: no wrap-up; continue to target depth."

    return user_content

```
=== END FILE: src/xsarena/core/prompt_runtime.py ===

=== START FILE: src/xsarena/core/v2_orchestrator/orchestrator.py ===
```python
"""Orchestrator for XSArena v0.2 - manages the overall workflow."""

import hashlib
import json
import os
import subprocess
from pathlib import Path
from typing import Any, Dict, List, Optional

from ..autopilot.fsm import AutopilotFSM
from ..backends import create_backend
from ..backends.transport import BackendTransport
from ..jobs.scheduler import Scheduler
from ..prompt import compose_prompt
from ..state import SessionState
from .specs import RunSpecV2


class Orchestrator:
    """Main orchestrator that manages the entire run process."""

    def __init__(self, transport: Optional[BackendTransport] = None) -> None:
        self.transport = transport
        self.fsm = AutopilotFSM()
        self._job_runner = None  # Lazy initialization to avoid circular import
        self.scheduler = Scheduler()

    def _calculate_directive_digests(
        self, overlays: List[str], extra_files: List[str]
    ) -> Dict[str, str]:
        """Calculate SHA256 digests for directive files and extra files."""
        digests = {}

        # Calculate digests for overlays
        for overlay in overlays:
            # Try to find overlay directive file
            overlay_paths = [
                Path(f"directives/style/{overlay}.md"),
                Path(f"directives/overlays/{overlay}.md"),
                Path(f"directives/{overlay}.md"),
            ]

            for overlay_path in overlay_paths:
                if overlay_path.exists():
                    try:
                        content = overlay_path.read_text(encoding="utf-8")
                        digest = hashlib.sha256(content.encode()).hexdigest()
                        digests[f"overlay:{overlay}"] = digest
                        break
                    except Exception:
                        continue

        # Calculate digests for extra files
        for extra_file in extra_files:
            extra_path = Path(extra_file)
            if extra_path.exists():
                try:
                    content = extra_path.read_text(encoding="utf-8")
                    digest = hashlib.sha256(content.encode()).hexdigest()
                    digests[f"extra:{extra_file}"] = digest
                except Exception:
                    continue

        # Calculate digest for base directive
        base_path = Path("directives/base/zero2hero.md")
        if base_path.exists():
            try:
                content = base_path.read_text(encoding="utf-8")
                digest = hashlib.sha256(content.encode()).hexdigest()
                digests["base:zero2hero"] = digest
            except Exception:
                pass

        return digests

    def _get_git_commit_hash(self) -> Optional[str]:
        """Get the current git commit hash."""
        try:
            result = subprocess.run(
                ["git", "rev-parse", "HEAD"],
                capture_output=True,
                text=True,
                cwd=os.getcwd(),
                check=True,
            )
            return result.stdout.strip()
        except (subprocess.CalledProcessError, FileNotFoundError):
            return None

    def _get_config_snapshot(self) -> Dict[str, Any]:
        """Get a snapshot of current configuration."""
        import yaml

        config_snapshot = {}

        # Get settings from config.yml
        config_path = Path(".xsarena/config.yml")
        if config_path.exists():
            try:
                with open(config_path, "r", encoding="utf-8") as f:
                    config_content = yaml.safe_load(f) or {}
                config_snapshot["settings"] = config_content.get("settings", {})
            except Exception:
                config_snapshot["settings"] = {}

        # Get bridge IDs from session state
        session_path = Path(".xsarena/session_state.json")
        if session_path.exists():
            try:
                with open(session_path, "r", encoding="utf-8") as f:
                    session_content = json.load(f) or {}
                # Only include bridge-related IDs
                bridge_ids = {}
                for key, value in session_content.items():
                    if "bridge" in key.lower():
                        bridge_ids[key] = value
                if bridge_ids:
                    config_snapshot["bridge_ids"] = bridge_ids
            except Exception:
                pass

        return config_snapshot

    def _check_directive_drift(self) -> List[str]:
        """Check for directive drift by comparing current directive files to the lockfile."""
        import json
        from pathlib import Path

        lockfile_path = Path(".xsarena/directives.lock")
        if not lockfile_path.exists():
            return []  # No lockfile exists, so no drift to check

        try:
            with open(lockfile_path, "r", encoding="utf-8") as f:
                lock_data = json.load(f)

            locked_directives = lock_data.get("directives", {})
            drifts = []

            for relative_path, expected_hash in locked_directives.items():
                file_path = Path(relative_path)
                if file_path.exists():
                    try:
                        content = file_path.read_text(encoding="utf-8")
                        current_hash = hashlib.sha256(content.encode()).hexdigest()
                        if current_hash != expected_hash:
                            drifts.append(f"Changed: {relative_path}")
                    except Exception:
                        drifts.append(f"Unreadable: {relative_path}")
                else:
                    drifts.append(f"Missing: {relative_path}")

            return drifts
        except Exception as e:
            print(f"Warning: Could not check directive drift: {e}")
            return []

    def _save_run_manifest(
        self,
        job_id: str,
        system_text: str,
        run_spec: RunSpecV2,
        overlays: List[str],
        extra_files: List[str],
    ) -> str:
        """Save run manifest with all required information."""
        import json
        from datetime import datetime
        from pathlib import Path

        # Calculate directive digests
        directive_digests = self._calculate_directive_digests(overlays, extra_files)

        # Get git commit hash
        git_commit_hash = self._get_git_commit_hash()

        # Get config snapshot
        config_snapshot = self._get_config_snapshot()

        # Check for directive drift and log if any
        directive_drifts = self._check_directive_drift()

        # Create manifest data
        manifest_data = {
            "final_system_text": system_text,
            "resolved_run_spec": run_spec.model_dump(),
            "directive_digests": directive_digests,
            "config_snapshot": config_snapshot,
            "git_commit_hash": git_commit_hash,
            "timestamp": datetime.now().isoformat(),
            "directive_drifts": directive_drifts,  # Include any detected drifts
        }

        # Create job directory if it doesn't exist
        job_dir = Path(".xsarena/jobs") / job_id
        job_dir.mkdir(parents=True, exist_ok=True)

        # Save manifest
        manifest_path = job_dir / "run_manifest.json"
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(manifest_data, f, indent=2, ensure_ascii=False)

        return str(manifest_path)

    def _get_timestamp(self) -> str:
        """Get current timestamp."""
        from datetime import datetime

        return datetime.now().isoformat()

    @property
    def job_runner(self):
        """Lazy load JobManager to avoid circular import."""
        if self._job_runner is None:
            from ..jobs.model import JobManager

            self._job_runner = JobManager()
        return self._job_runner

    async def run_spec(
        self, run_spec: RunSpecV2, backend_type: str = "bridge", priority: int = 5
    ) -> str:
        """Run a specification through the orchestrator."""
        if not self.transport:
            # Pass bridge-specific IDs if they are provided in the run spec
            transport_kwargs = {}
            if run_spec.bridge_session_id:
                transport_kwargs["session_id"] = run_spec.bridge_session_id
            if run_spec.bridge_message_id:
                transport_kwargs["message_id"] = run_spec.bridge_message_id

            self.transport = create_backend(backend_type, **transport_kwargs)

        # session + prompt composition (unchanged)
        session_state = SessionState.load_from_file(".xsarena/session_state.json")
        resolved = run_spec.resolved()
        resolved["min_length"] = getattr(
            session_state, "output_min_chars", resolved["min_length"]
        )

        # compose system_text here as you already do:
        comp = compose_prompt(
            subject=run_spec.subject,
            base="zero2hero",
            overlays=run_spec.overlays,
            extra_notes=run_spec.extra_note,
            min_chars=resolved["min_length"],
            passes=resolved["passes"],
            max_chunks=resolved["chunks"],
            apply_reading_overlay=getattr(session_state, "reading_overlay_on", False),
        )
        system_text = comp.system_text
        for file_path in run_spec.extra_files:
            p = Path(file_path)
            if p.exists():
                system_text += "\n\n" + p.read_text(encoding="utf-8", errors="ignore")

        # NEW: resume-safe scheduling
        out_path = (
            run_spec.out_path
            or f"./books/{run_spec.subject.replace(' ', '_')}.final.md"
        )

        # Check if output file already exists and prompt user if running in TTY
        import sys

        if Path(out_path).exists() and sys.stdin.isatty():
            print(f"Output file already exists: {out_path}")
            response = input("Resume (R) or Overwrite (O)? [R/O]: ").strip().upper()
            if response == "O":
                # User chose to overwrite, so don't resume
                job_id = self.job_runner.submit(
                    run_spec, backend_type, system_text, session_state
                )
                print(f"[run] submitted (overwrite) → {job_id}")
            elif response == "R":
                # User chose to resume, check for existing job
                existing = self.job_runner.find_resumable_job_by_output(out_path)
                if existing:
                    job = self.job_runner.load(existing)
                    if job.state == "RUNNING":
                        job_id = job.id
                        print(f"[run] existing RUNNING job → {job_id}")
                    else:
                        job_id = self.job_runner.prepare_job_for_resume(existing)
                        print(f"[run] resuming job → {job_id}")
                else:
                    # No existing job found, submit a new one
                    job_id = self.job_runner.submit(
                        run_spec, backend_type, system_text, session_state
                    )
                    print(f"[run] submitted → {job_id}")
            else:
                # Default to resume behavior if user doesn't enter O
                existing = self.job_runner.find_resumable_job_by_output(out_path)
                if existing:
                    job = self.job_runner.load(existing)
                    if job.state == "RUNNING":
                        job_id = job.id
                        print(f"[run] existing RUNNING job → {job_id}")
                    else:
                        job_id = self.job_runner.prepare_job_for_resume(existing)
                        print(f"[run] resuming job → {job_id}")
                else:
                    # No existing job found, submit a new one
                    job_id = self.job_runner.submit(
                        run_spec, backend_type, system_text, session_state
                    )
                    print(f"[run] submitted → {job_id}")
        else:
            # File doesn't exist or not in TTY, proceed with normal resume logic
            existing = self.job_runner.find_resumable_job_by_output(out_path)
            if existing:
                job = self.job_runner.load(existing)
                if job.state == "RUNNING":
                    job_id = job.id
                    print(f"[run] existing RUNNING job → {job_id}")
                else:
                    job_id = self.job_runner.prepare_job_for_resume(existing)
                    print(f"[run] resuming job → {job_id}")
            else:
                job_id = self.job_runner.submit(
                    run_spec, backend_type, system_text, session_state
                )
                print(f"[run] submitted → {job_id}")

        # Save run manifest with all required information
        self._save_run_manifest(
            job_id=job_id,
            system_text=system_text,
            run_spec=run_spec,
            overlays=run_spec.overlays,
            extra_files=run_spec.extra_files,
        )
        print(f"[run] manifest saved → .xsarena/jobs/{job_id}/run_manifest.json")

        self.scheduler.set_transport(self.transport)
        await self.scheduler.submit_job(job_id, priority=priority)
        await self.scheduler.wait_for_job(job_id)
        return job_id

    async def run_with_fsm(self, run_spec: RunSpecV2) -> Dict[str, Any]:
        """Run a specification using the FSM approach."""
        # Convert run_spec to FSM-compatible format
        fsm_input = run_spec.model_dump()

        # Run the FSM
        result = await self.fsm.run(fsm_input)

        return result.model_dump()

    async def run_continue(
        self,
        run_spec: RunSpecV2,
        file_path: str,
        until_end: bool = False,
        priority: int = 5,
    ) -> str:
        """Run a continue operation from an existing file."""
        # Create a transport if not provided
        if not self.transport:
            # Pass bridge-specific IDs if they are provided in the run spec
            transport_kwargs = {}
            if run_spec.bridge_session_id:
                transport_kwargs["session_id"] = run_spec.bridge_session_id
            if run_spec.bridge_message_id:
                transport_kwargs["message_id"] = run_spec.bridge_message_id

            self.transport = create_backend("bridge", **transport_kwargs)

        # Load session state to override run_spec values with dynamic config
        session_state = SessionState.load_from_file(".xsarena/session_state.json")

        # Create a modified run_spec with values from session state
        resolved = run_spec.resolved()

        # Override resolved values with session state values if they exist
        resolved["min_length"] = getattr(
            session_state, "output_min_chars", resolved["min_length"]
        )

        composition = compose_prompt(
            subject=run_spec.subject,
            base="zero2hero",  # Default base, can be made configurable
            overlays=run_spec.overlays,
            extra_notes=run_spec.extra_note,
            min_chars=resolved["min_length"],
            passes=resolved["passes"],
            max_chunks=resolved["chunks"],
            apply_reading_overlay=getattr(session_state, "reading_overlay_on", False),
        )

        # Read contents of extra_files and append to system_text
        system_text = composition.system_text
        for extra_file_path in run_spec.extra_files:
            try:
                p = Path(extra_file_path)
                if p.exists():
                    content = p.read_text(encoding="utf-8")
                    system_text += "\n\n" + content
            except Exception as e:
                print(f"Warning: Could not read extra file {extra_file_path}: {e}")

        # NEW: resume-safe scheduling for continue operations
        out_path = file_path  # For continue, the file_path is the output path

        # Check if output file already exists and prompt user if running in TTY
        import sys

        if Path(out_path).exists() and sys.stdin.isatty():
            print(f"Output file already exists: {out_path}")
            response = input("Resume (R) or Overwrite (O)? [R/O]: ").strip().upper()
            if response == "O":
                # User chose to overwrite, so don't resume
                job_id = self.job_runner.submit_continue(
                    run_spec, file_path, until_end, system_text, session_state
                )
                print(f"[run] submitted (overwrite) → {job_id}")
            elif response == "R":
                # User chose to resume, check for existing job
                existing = self.job_runner.find_resumable_job_by_output(out_path)
                if existing:
                    job = self.job_runner.load(existing)
                    if job.state == "RUNNING":
                        job_id = job.id
                        print(f"[run] existing RUNNING job → {job_id}")
                    else:
                        job_id = self.job_runner.prepare_job_for_resume(existing)
                        print(f"[run] resuming job → {job_id}")
                else:
                    # No existing job found, submit a new continue job
                    job_id = self.job_runner.submit_continue(
                        run_spec, file_path, until_end, system_text, session_state
                    )
                    print(f"[run] submitted → {job_id}")
            else:
                # Default to resume behavior if user doesn't enter O
                existing = self.job_runner.find_resumable_job_by_output(out_path)
                if existing:
                    job = self.job_runner.load(existing)
                    if job.state == "RUNNING":
                        job_id = job.id
                        print(f"[run] existing RUNNING job → {job_id}")
                    else:
                        job_id = self.job_runner.prepare_job_for_resume(existing)
                        print(f"[run] resuming job → {job_id}")
                else:
                    # No existing job found, submit a new continue job
                    job_id = self.job_runner.submit_continue(
                        run_spec, file_path, until_end, system_text, session_state
                    )
                    print(f"[run] submitted → {job_id}")
        else:
            # File doesn't exist or not in TTY, proceed with normal resume logic
            existing = self.job_runner.find_resumable_job_by_output(out_path)
            if existing:
                job = self.job_runner.load(existing)
                if job.state == "RUNNING":
                    job_id = job.id
                    print(f"[run] existing RUNNING job → {job_id}")
                else:
                    job_id = self.job_runner.prepare_job_for_resume(existing)
                    print(f"[run] resuming job → {job_id}")
            else:
                # Submit job to the new system with the composed system_text and session state
                job_id = self.job_runner.submit_continue(
                    run_spec, file_path, until_end, system_text, session_state
                )
                print(f"[run] submitted → {job_id}")

        # Save run manifest with all required information
        self._save_run_manifest(
            job_id=job_id,
            system_text=system_text,
            run_spec=run_spec,
            overlays=run_spec.overlays,
            extra_files=run_spec.extra_files,
        )
        print(f"[run] manifest saved → .xsarena/jobs/{job_id}/run_manifest.json")

        # Set the transport for the scheduler
        self.scheduler.set_transport(self.transport)

        # Submit job to scheduler
        await self.scheduler.submit_job(job_id, priority=priority)

        # Wait for job to complete
        await self.scheduler.wait_for_job(job_id)

        return job_id

```
=== END FILE: src/xsarena/core/v2_orchestrator/orchestrator.py ===

=== START FILE: src/xsarena/core/v2_orchestrator/specs.py ===
```python
"""Run specification model for XSArena v0.2."""

from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field


class LengthPreset(str, Enum):
    """Length presets for runs."""

    STANDARD = "standard"
    LONG = "long"
    VERY_LONG = "very-long"
    MAX = "max"


class SpanPreset(str, Enum):
    """Span presets for runs."""

    MEDIUM = "medium"
    LONG = "long"
    BOOK = "book"


class RunSpecV2(BaseModel):
    """Version 2 run specification with typed fields."""

    subject: str = Field(..., description="The subject to generate content about")
    length: LengthPreset = Field(
        LengthPreset.LONG, description="Length preset for the run"
    )
    span: SpanPreset = Field(SpanPreset.BOOK, description="Span preset for the run")
    overlays: List[str] = Field(
        default_factory=lambda: ["narrative", "no_bs"],
        description="Overlay specifications",
    )
    extra_note: str = Field("", description="Additional notes or instructions")
    extra_files: List[str] = Field(
        default_factory=list, description="Additional files to include"
    )
    out_path: Optional[str] = Field(None, description="Output path for the result")
    outline_scaffold: Optional[str] = Field(
        None, description="Outline scaffold to follow"
    )
    generate_plan: bool = Field(False, description="Generate an outline first.")
    window_size: Optional[int] = Field(
        None, description="History window size for this run."
    )

    # Additional fields that might be needed
    profile: Optional[str] = Field(None, description="Profile to use for the run")
    backend: Optional[str] = Field("bridge", description="Backend to use for the run")
    model: Optional[str] = Field("default", description="Model to use for the run")
    concurrency: int = Field(1, description="Number of concurrent operations")
    timeout: int = Field(300, description="Timeout for operations in seconds")

    # Bridge-specific configuration
    bridge_session_id: Optional[str] = Field(
        None, description="Specific session ID for bridge"
    )
    bridge_message_id: Optional[str] = Field(
        None, description="Specific message ID for bridge"
    )

    class Config:
        """Configuration for the model."""

        extra = "forbid"  # Forbid extra fields to catch typos

    def resolved(self) -> Dict[str, Any]:
        """Resolve presets to actual values."""
        length_presets = {
            "standard": {"min": 4200, "passes": 1},
            "long": {"min": 5800, "passes": 3},
            "very-long": {"min": 6200, "passes": 4},
            "max": {"min": 6800, "passes": 5},
        }

        span_presets = {"medium": 12, "long": 24, "book": 40}

        length_config = length_presets[self.length.value]
        chunks = span_presets[self.span.value]

        return {
            "min_length": length_config["min"],
            "passes": length_config["passes"],
            "chunks": chunks,
        }

```
=== END FILE: src/xsarena/core/v2_orchestrator/specs.py ===

=== START FILE: src/xsarena/core/jobs/model.py ===
```python
"""Job manager facade for XSArena v0.3."""

import asyncio
import json
import os
import uuid
from datetime import datetime
from typing import Any, Awaitable, Callable, Dict, List, Optional

from pydantic import BaseModel

from ..backends.transport import BackendTransport, BaseEvent
from ..v2_orchestrator.specs import RunSpecV2

# Needed for exception mapping in map_exception_to_error_code
try:
    import aiohttp  # type: ignore
except Exception:  # pragma: no cover
    aiohttp = None  # type: ignore
try:
    import requests  # type: ignore
except Exception:  # pragma: no cover
    requests = None  # type: ignore


def map_exception_to_error_code(exception: Exception) -> str:
    """Map common exceptions to standardized error codes."""
    error_map = {
        # Configuration errors
        KeyError: "invalid_config",  # When config keys are missing
        ValueError: "invalid_config",  # When config values are invalid
        FileNotFoundError: "file_not_found",
        # Content/Processing errors
        UnicodeDecodeError: "encoding_error",
        json.JSONDecodeError: "json_error",
    }

    # Add aiohttp-specific mappings only if aiohttp is available
    if aiohttp is not None:
        error_map.update(
            {
                aiohttp.ClientError: "transport_unavailable",
                aiohttp.ClientConnectorError: "transport_unavailable",
                aiohttp.ServerTimeoutError: "transport_timeout",
                aiohttp.ClientResponseError: "api_error",
            }
        )

    # Add requests-specific mappings only if requests is available
    if requests is not None:
        error_map.update(
            {
                requests.ConnectionError: "transport_unavailable",
                requests.Timeout: "transport_timeout",
            }
        )

    # Always include the base ConnectionError
    error_map[ConnectionError] = "transport_unavailable"

    # Check if it's a specific HTTP error with status code
    if hasattr(exception, "status") and isinstance(exception.status, int):
        status_code = exception.status
        if status_code == 401 or status_code == 403:
            return "auth_error"
        elif status_code == 429:
            return "quota_exceeded"
        elif status_code >= 500:
            return "server_error"
        elif status_code >= 400:
            return "api_error"

    # Try to match exact exception type
    for exc_type, error_code in error_map.items():
        if isinstance(exception, exc_type):
            return error_code

    # Check by name if exact type doesn't match
    exc_name = type(exception).__name__
    if "auth" in exc_name.lower() or "Auth" in exc_name:
        return "auth_error"
    elif (
        "quota" in exc_name.lower()
        or "Quota" in exc_name
        or "limit" in exc_name.lower()
    ):
        return "quota_exceeded"
    elif "connection" in exc_name.lower() or "Connection" in exc_name:
        return "transport_unavailable"
    elif "timeout" in exc_name.lower() or "Timeout" in exc_name:
        return "transport_timeout"

    # Default error code
    return "unknown_error"


def get_user_friendly_error_message(error_code: str) -> str:
    """Get user-friendly error message for an error code."""
    messages = {
        "transport_unavailable": "Transport unavailable - check network connection or backend status",
        "transport_timeout": "Request timed out - backend may be slow to respond",
        "auth_error": "Authentication failed - check API key or credentials",
        "quota_exceeded": "Quota exceeded - rate limit reached or account limit exceeded",
        "api_error": "API error - backend returned an error response",
        "server_error": "Server error - backend temporarily unavailable",
        "invalid_config": "Invalid configuration - check your settings",
        "file_not_found": "File not found - check the file path",
        "encoding_error": "Encoding error - file contains invalid characters",
        "json_error": "JSON parsing error - check file format",
        "unknown_error": "An unknown error occurred",
    }
    return messages.get(error_code, "An error occurred")


class JobV3(BaseModel):
    """Version 3 job model with typed fields."""

    id: str
    name: str
    run_spec: RunSpecV2
    backend: str
    state: str = "PENDING"  # PENDING/RUNNING/STALLED/RETRYING/DONE/FAILED/CANCELLED
    retries: int = 0
    created_at: str = datetime.now().isoformat()
    updated_at: str = datetime.now().isoformat()
    artifacts: Dict[str, str] = {}
    meta: Dict[str, Any] = {}
    progress: Dict[str, Any] = {}  # Track progress like chunks completed, tokens used


from .store import JobStore


class JobManager:
    """Version 3 job manager facade with typed events and async event bus."""

    def __init__(self, project_defaults: Optional[Dict[str, Any]] = None):
        self.defaults = project_defaults or {}
        self.job_store = JobStore()
        # Import JobExecutor locally to avoid circular import
        from .executor_core import JobExecutor
        self.executor = JobExecutor(self.job_store)
        self.event_handlers: List[Callable[[BaseEvent], Awaitable[None]]] = []
        # Add control_queues attribute for compatibility with tests
        self.control_queues = self.executor.control_queues

    def register_event_handler(self, handler: Callable[[BaseEvent], Awaitable[None]]):
        """Register an event handler for job events."""
        self.event_handlers.append(handler)

    async def _emit_event(self, event: BaseEvent):
        """Emit an event to all registered handlers."""
        for handler in self.event_handlers:
            try:
                await handler(event)
            except Exception:
                # Log error but don't fail the job due to event handler issues
                pass

    def submit(
        self,
        run_spec: "RunSpecV2",
        backend: str = "bridge",
        system_text: str = "",
        session_state: Optional["SessionState"] = None,
    ) -> str:
        """Submit a new job with the given run specification. If a job with same output file exists and is incomplete, resume from last completed chunk."""
        # Check if a job with the same output file already exists
        out_path = (
            run_spec.out_path
            or f"./books/{run_spec.subject.replace(' ', '_')}.final.md"
        )

        # Look for existing jobs with the same output file
        existing_job_id = None
        for job in self.list_jobs():
            if job.run_spec.out_path == out_path or (
                not job.run_spec.out_path
                and out_path.endswith(
                    job.run_spec.subject.replace(" ", "_") + ".final.md"
                )
            ):
                # Check if the job is incomplete (not in DONE, FAILED, or CANCELLED state)
                if job.state not in ["DONE", "FAILED", "CANCELLED"]:
                    existing_job_id = job.id
                    break

        if existing_job_id:
            # Resume the existing job
            last_chunk = self.job_store._get_last_completed_chunk(existing_job_id)

            # Update job state to PENDING to restart processing
            job = self.load(existing_job_id)
            job.state = "PENDING"
            job.updated_at = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
            self.job_store.save(job)

            # Log resume event
            resume_event = {
                "type": "job_resumed_from_chunk",
                "last_completed_chunk": last_chunk,
                "resuming_from_chunk": last_chunk + 1,
            }
            self.job_store._log_event(existing_job_id, resume_event)

            return existing_job_id
        else:
            # Create a new job as before
            job_id = str(uuid.uuid4())
            job = JobV3(
                id=job_id,
                name=run_spec.subject,
                run_spec=run_spec,
                backend=backend,
                state="PENDING",
                meta=(
                    {
                        "system_text": system_text,
                        "session_state": (
                            session_state.to_dict() if session_state else {}
                        ),
                    }
                    if system_text or session_state
                    else {
                        "session_state": (
                            session_state.to_dict() if session_state else {}
                        )
                    }
                ),
            )
            jd = self.job_store._job_dir(job_id)
            jd.mkdir(parents=True, exist_ok=True)

            # Save job metadata
            self.job_store.save(job)

            # Initialize events log
            event_data = {
                "ts": datetime.now().strftime("%Y-%m-%dT%H:%M:%S"),
                "type": "job_submitted",
                "job_id": job_id,
                "spec": run_spec.model_dump(),
            }
            self.job_store._log_event(job_id, event_data)

            return job_id

    def load(self, job_id: str) -> JobV3:
        """Load a job by ID."""
        return self.job_store.load(job_id)

    def _save_job(self, job: JobV3):
        """Save job metadata."""
        self.job_store.save(job)

    def _ts(self) -> str:
        """Get current timestamp."""
        from datetime import datetime

        return datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

    def list_jobs(self) -> List[JobV3]:
        """List all jobs."""
        return self.job_store.list_all()

    def _log_event(self, job_id: str, ev: Dict[str, Any]):
        """Log an event for a job with standardized structure."""
        self.job_store._log_event(job_id, ev)

    def submit_continue(
        self,
        run_spec: "RunSpecV2",
        file_path: str,
        until_end: bool = False,
        system_text: str = "",
        session_state: Optional["SessionState"] = None,
    ) -> str:
        """Submit a continue job with the given run specification and file path."""
        job_id = str(uuid.uuid4())
        job = JobV3(
            id=job_id,
            name=f"Continue: {run_spec.subject}",
            run_spec=run_spec,
            backend=run_spec.backend or "bridge",
            state="PENDING",
            meta=(
                {
                    "continue_from_file": file_path,
                    "until_end": until_end,
                    "system_text": system_text,
                    "session_state": session_state.to_dict() if session_state else {},
                }
                if system_text or session_state
                else {
                    "continue_from_file": file_path,
                    "until_end": until_end,
                    "session_state": session_state.to_dict() if session_state else {},
                }
            ),
        )
        jd = self.job_store._job_dir(job_id)
        jd.mkdir(parents=True, exist_ok=True)

        # Save job metadata
        self.job_store.save(job)

        # Initialize events log
        event_data = {
            "ts": datetime.now().strftime("%Y-%m-%dT%H:%M:%S"),
            "type": "job_submitted",
            "job_id": job_id,
            "spec": run_spec.model_dump(),
            "continue_from_file": file_path,
            "until_end": until_end,
        }
        self.job_store._log_event(job_id, event_data)

        return job_id

    def _normalize_out(self, run_spec) -> str:
        """Mirror how run spec/out_path is constructed in orchestrator/run."""
        base = (
            run_spec.out_path
            or f"./books/{run_spec.subject.replace(' ', '_')}.final.md"
        )
        return os.path.abspath(base)

    def find_resumable_job_by_output(self, out_path: str) -> Optional[str]:
        """Find an existing job (not DONE/FAILED/CANCELLED) that targets out_path."""
        return self.job_store.find_resumable(out_path)

    def prepare_job_for_resume(self, job_id: str) -> str:
        """Set job to PENDING and log; used to requeue a non-running job."""
        job = self.load(job_id)
        job.state = "PENDING"
        job.updated_at = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
        self._save_job(job)
        self._log_event(job_id, {"type": "job_prepared_for_resume"})
        return job_id

    async def run_job(
        self,
        job_id: str,
        transport: BackendTransport,
        control_queue: asyncio.Queue = None,
        resume_event: asyncio.Event = None,
    ):
        """Run a job by delegating its execution to the JobExecutor."""
        job = self.load(job_id)

        # The executor requires an on_event callback. We'll define a simple one
        # here that emits events to any registered handlers.
        async def on_event_handler(event: BaseEvent):
            await self._emit_event(event)

        # Use the executor's control queues and resume events to ensure consistency
        # Use provided control queue and resume event if available, otherwise create new ones
        if control_queue is not None:
            self.executor.control_queues[job_id] = control_queue
        elif job_id not in self.executor.control_queues:
            self.executor.control_queues[job_id] = asyncio.Queue()

        if resume_event is not None:
            self.executor.resume_events[job_id] = resume_event
        elif job_id not in self.executor.resume_events:
            self.executor.resume_events[job_id] = asyncio.Event()
            self.executor.resume_events[job_id].set()  # Initially not paused

        # Delegate the entire run loop to the executor.
        await self.executor.run(
            job=job,
            transport=transport,
            on_event=on_event_handler,
            control_queue=self.executor.control_queues[job_id],
            resume_event=self.executor.resume_events[job_id],
        )

    async def send_control_message(self, job_id: str, command: str, text: str = None):
        """Send a control message to a running job."""
        # Use the executor's control queues
        if job_id not in self.executor.control_queues:
            self.executor.control_queues[job_id] = asyncio.Queue()

        message = {"type": command}
        if text:
            message["text"] = text

        await self.executor.control_queues[job_id].put(message)

        # Log the control command
        self._log_event(
            job_id, {"type": f"control_{command}", "command": command, "text": text}
        )

    async def wait_for_job_completion(self, job_id: str):
        """Wait for a job to reach a terminal state (DONE/FAILED/CANCELLED)."""

        while True:
            job = self.load(job_id)
            if job.state in ["DONE", "FAILED", "CANCELLED"]:
                break
            # Check every 2 seconds
            await asyncio.sleep(2.0)

        import logging

        logger = logging.getLogger(__name__)

        # Log final status
        if job.state == "DONE":
            logger.info(f"[run] Job {job_id} completed successfully")
        elif job.state == "FAILED":
            # Try to get more detailed error information from events
            error_message = self._get_last_error_message(job_id)
            if error_message:
                logger.error(f"[run] Job {job_id} failed: {error_message}")
            else:
                logger.error(f"[run] Job {job_id} failed")
        elif job.state == "CANCELLED":
            logger.info(f"[run] Job {job_id} cancelled")

    def _get_last_error_message(self, job_id: str) -> str:
        """Get the last error message from job events."""
        import json

        events_file = self.job_store._events_path(job_id)
        if not events_file.exists():
            return ""

        # Read the last few lines of the events file to find error events
        try:
            with open(events_file, "r", encoding="utf-8") as f:
                lines = f.readlines()

            # Look at the last 10 lines to find error events
            for line in reversed(lines[-10:]):
                try:
                    event = json.loads(line.strip())
                    if event.get("type") == "error" and "user_message" in event:
                        return event["user_message"]
                    elif event.get("type") == "error" and "message" in event:
                        return event["message"]
                    elif event.get("type") == "job_failed" and "error" in event:
                        return str(event["error"])
                except (json.JSONDecodeError, KeyError):
                    continue

            # If no specific error message found, check for any error-related events
            for line in reversed(lines[-20:]):  # Check more lines if needed
                try:
                    event = json.loads(line.strip())
                    if "error" in event or event.get("type") == "error":
                        message = event.get(
                            "user_message", event.get("message", event.get("error", ""))
                        )
                        if message:
                            return str(message)
                except (json.JSONDecodeError, KeyError):
                    continue

        except Exception:
            # If there's an issue reading the events file, return empty string
            pass

        return ""

```
=== END FILE: src/xsarena/core/jobs/model.py ===

=== START FILE: src/xsarena/core/jobs/executor.py ===
```python
"""Job execution layer for XSArena v0.3."""

```
=== END FILE: src/xsarena/core/jobs/executor.py ===

=== START FILE: src/xsarena/core/jobs/scheduler.py ===
```python
"""Scheduler for XSArena jobs with concurrency and quiet hours."""

import asyncio
import contextlib
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml

from ...utils.io import atomic_write
from ..backends.transport import BackendTransport
from ..project_config import get_project_settings
from .model import JobManager


class Scheduler:
    """Job scheduler with concurrency control and quiet hours."""

    def __init__(
        self, max_concurrent: int = 1, quiet_hours: Optional[Dict[str, tuple]] = None
    ):
        self.max_concurrent = max_concurrent
        self.quiet_hours = quiet_hours or {}
        self.running_jobs: Dict[str, asyncio.Task] = {}
        self.job_queue: List[
            tuple[int, str]
        ] = (
            []
        )  # Queue of (priority, job ID) tuples, where lower number = higher priority
        self.transport: Optional[BackendTransport] = None
        self._project = self._load_project_cfg()
        self._project_settings = get_project_settings()

        # Initialize and load persisted queue
        self._queue_file = Path(".xsarena/ops/queue.json")
        self._load_persisted_queue()

    def _load_project_cfg(self) -> Dict[str, Any]:
        p = Path(".xsarena/project.yml")
        if p.exists():
            try:
                return yaml.safe_load(p.read_text(encoding="utf-8")) or {}
            except Exception:
                return {}
        return {}

    def set_transport(self, transport: BackendTransport):
        """Set the transport for the scheduler."""
        self.transport = transport

    def is_quiet_time(self) -> bool:
        """Check if it's currently quiet hours."""
        cfg = (self._project.get("scheduler") or {}).get("quiet_hours") or {}
        if not cfg.get("enabled", False):
            return False

        now = datetime.now()
        day = now.strftime("%A").lower()
        current_hour = now.hour

        if day in cfg:
            start_hour, end_hour = (
                cfg[day]
                if isinstance(cfg[day], (list, tuple))
                else (cfg.get("start", 0), cfg.get("end", 0))
            )
            if start_hour <= current_hour < end_hour:
                return True
            elif start_hour > end_hour:  # Overnight hours (e.g., 22 to 6)
                if current_hour >= start_hour or current_hour < end_hour:
                    return True

        return False

    async def submit_job(self, job_id: str, priority: int = 5) -> bool:
        """Submit a job to the scheduler with a priority (lower number = higher priority)."""
        if self.is_quiet_time():
            # Add to queue for later processing
            self.job_queue.append((priority, job_id))
            self._sort_queue()  # Keep queue sorted by priority
            self._persist_queue()
            return True

        # Check both total and backend-specific limits
        backend_type = self._get_backend_for_job(job_id)
        backend_limit = self._get_concurrent_limit_for_backend(backend_type)
        current_backend_jobs = self._get_running_jobs_for_backend(backend_type)

        if (
            len(self.running_jobs) < self.effective_max_concurrent
            and current_backend_jobs < backend_limit
        ):
            # Run immediately
            task = asyncio.create_task(self._run_job(job_id))
            self.running_jobs[job_id] = task
            return True
        else:
            # Add to queue
            self.job_queue.append((priority, job_id))
            self._sort_queue()  # Keep queue sorted by priority
            self._persist_queue()
            return True

    def _sort_queue(self):
        """Sort the job queue by priority (lower number = higher priority)."""
        self.job_queue.sort(
            key=lambda x: x[0]
        )  # Sort by priority (first element of tuple)

    async def _run_job(self, job_id: str):
        """Internal method to run a job."""
        if not self.transport:
            raise ValueError("Transport not set for scheduler")

        # Create a job runner and run the job
        runner = JobManager()

        # Create control queue and resume event for this job
        control_queue = asyncio.Queue()
        resume_event = asyncio.Event()
        resume_event.set()  # Initially not paused

        try:
            await runner.run_job(job_id, self.transport, control_queue, resume_event)
        finally:
            # Remove from running jobs when done
            if job_id in self.running_jobs:
                del self.running_jobs[job_id]

            # Check if there are queued jobs to run
            await self._process_queue()

    async def _process_queue(self):
        """Process queued jobs if there's capacity."""
        # Load any external changes to the queue from file
        self._load_persisted_queue()

        # Process jobs that can run within limits
        remaining_queue = []
        for priority, job_id in self.job_queue:
            if self.is_quiet_time():
                remaining_queue.append(
                    (priority, job_id)
                )  # Keep in queue during quiet hours
                continue

            backend_type = self._get_backend_for_job(job_id)
            backend_limit = self._get_concurrent_limit_for_backend(backend_type)
            current_backend_jobs = self._get_running_jobs_for_backend(backend_type)

            if (
                len(self.running_jobs) < self.effective_max_concurrent
                and current_backend_jobs < backend_limit
            ):
                # Start this job
                task = asyncio.create_task(self._run_job(job_id))
                self.running_jobs[job_id] = task
            else:
                # Keep in queue
                remaining_queue.append((priority, job_id))

        self.job_queue = remaining_queue
        self._persist_queue()  # Persist the updated queue

    async def wait_for_job(self, job_id: str):
        """Wait for a specific job to complete."""
        if job_id in self.running_jobs:
            await self.running_jobs[job_id]

    async def cancel_job(self, job_id: str) -> bool:
        """Cancel a running job."""
        if job_id in self.running_jobs:
            task = self.running_jobs[job_id]
            task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await task
            del self.running_jobs[job_id]
            return True
        else:
            # Look for the job in the priority queue
            for i, (priority, queued_job_id) in enumerate(self.job_queue):
                if queued_job_id == job_id:
                    self.job_queue.pop(i)
                    self._persist_queue()  # Persist the updated queue
                    return True
        return False

    def _get_backend_for_job(self, job_id: str) -> str:
        """Get the backend type for a specific job."""
        try:
            from .model import JobManager

            runner = JobManager()
            job = runner.load(job_id)
            return job.backend
        except Exception:
            # Default to bridge if we can't determine the backend
            return "bridge"

    def _load_persisted_queue(self):
        """Load the persisted job queue from file."""
        if self._queue_file.exists():
            try:
                import json

                content = self._queue_file.read_text(encoding="utf-8")
                data = json.loads(content)

                # Handle both old format (list of job IDs) and new format (list of [priority, job_id] tuples)
                raw_queue = data.get("queue", [])

                if not raw_queue:
                    self.job_queue = []
                    return

                # Check if this is the old format (just job IDs) or new format (priority, job_id pairs)
                if raw_queue and isinstance(raw_queue[0], str):
                    # Old format: just job IDs, assign default priority
                    self.job_queue = [(5, job_id) for job_id in raw_queue]
                elif (
                    raw_queue
                    and isinstance(raw_queue[0], list)
                    and len(raw_queue[0]) == 2
                ):
                    # New format: [priority, job_id] pairs
                    self.job_queue = [
                        (priority, job_id) for priority, job_id in raw_queue
                    ]
                else:
                    # Unexpected format, use default
                    self.job_queue = []
                    return

                # Filter out jobs that no longer exist
                valid_jobs = []
                for priority, job_id in self.job_queue:
                    try:
                        from .model import JobManager

                        runner = JobManager()
                        job = runner.load(job_id)
                        # Only keep PENDING jobs
                        if job.state == "PENDING":
                            valid_jobs.append((priority, job_id))
                    except Exception:
                        # Job doesn't exist anymore, skip it
                        continue

                self.job_queue = valid_jobs
                self._sort_queue()  # Sort by priority
            except Exception as e:
                # If there's an error loading the queue, start fresh
                print(f"Warning: Could not load persisted queue: {e}")
                self.job_queue = []

    def _persist_queue(self):
        """Persist the current job queue to file."""
        self._queue_file.parent.mkdir(parents=True, exist_ok=True)
        import json

        # Convert priority tuples to list format for JSON serialization
        queue_for_json = [[priority, job_id] for priority, job_id in self.job_queue]

        data = {"queue": queue_for_json, "timestamp": datetime.now().isoformat()}
        atomic_write(self._queue_file, json.dumps(data, indent=2), encoding="utf-8")

    def _get_concurrent_limit_for_backend(self, backend_type: str) -> int:
        """Get the concurrent job limit for a specific backend type."""
        settings = self._project_settings
        if backend_type == "bridge":
            return settings.concurrency.bridge
        elif backend_type == "openrouter":
            return settings.concurrency.openrouter
        else:
            # Default to bridge limit for other types
            return settings.concurrency.bridge

    def _get_running_jobs_for_backend(self, backend_type: str) -> int:
        """Get the number of currently running jobs for a specific backend type."""
        count = 0
        for job_id in self.running_jobs:
            job_backend = self._get_backend_for_job(job_id)
            if job_backend == backend_type:
                count += 1
        return count

    @property
    def effective_max_concurrent(self) -> int:
        """Get the effective max concurrent jobs from config or default."""
        # For now, return the total limit
        return self._project_settings.concurrency.total

    def get_status(self) -> Dict[str, Any]:
        """Get scheduler status."""
        return {
            "max_concurrent": self.effective_max_concurrent,
            "running_jobs": len(self.running_jobs),
            "queued_jobs": len(self.job_queue),
            "is_quiet_time": self.is_quiet_time(),
            "running_job_ids": list(self.running_jobs.keys()),
            "queued_job_ids": [
                job_id for priority, job_id in self.job_queue
            ],  # Just the job IDs
            "queued_jobs_with_priority": [
                [priority, job_id] for priority, job_id in self.job_queue
            ],  # Priority and job ID pairs
        }

    async def run_scheduler(self):
        """Main scheduler loop - runs indefinitely."""
        while True:
            # Process queued jobs if there's capacity
            await self._process_queue()

            # Wait before checking again
            await asyncio.sleep(1)

```
=== END FILE: src/xsarena/core/jobs/scheduler.py ===

=== START FILE: src/xsarena/core/jobs/store.py ===
```python
"""Job persistence layer for XSArena v0.3."""

import contextlib
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from ...utils.helpers import load_json_with_error_handling
from ...utils.io import atomic_write
from .model import JobV3


def load_json(path: Path) -> dict:
    """Helper to load JSON with error handling."""
    return load_json_with_error_handling(path)


class JobStore:
    """Handles job persistence operations (load/save/list)."""

    def __init__(self):
        Path(".xsarena/jobs").mkdir(parents=True, exist_ok=True)

    def _job_dir(self, job_id: str) -> Path:
        """Get the directory for a job."""
        return Path(".xsarena") / "jobs" / job_id

    def _get_last_completed_chunk(self, job_id: str) -> int:
        """Get the index of the last completed chunk by parsing events.jsonl."""
        events_path = self._job_dir(job_id) / "events.jsonl"
        if not events_path.exists():
            return 0

        last_chunk_idx = 0
        try:
            with open(events_path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        try:
                            event = json.loads(line.strip())
                            if (
                                event.get("type") == "chunk_done"
                                and "chunk_idx" in event
                            ):
                                chunk_idx = event["chunk_idx"]
                                if chunk_idx > last_chunk_idx:
                                    last_chunk_idx = chunk_idx
                        except json.JSONDecodeError:
                            continue
        except Exception:
            pass  # If we can't read the file, return 0

        return last_chunk_idx

    def load(self, job_id: str) -> JobV3:
        """Load a job by ID."""
        job_path = self._job_dir(job_id) / "job.json"
        data = load_json(job_path)
        return JobV3(**data)

    def save(self, job: JobV3):
        """Save job metadata."""
        jd = self._job_dir(job.id)
        job_path = jd / "job.json"
        atomic_write(job_path, json.dumps(job.model_dump(), indent=2), encoding="utf-8")

    def list_all(self) -> List[JobV3]:
        """List all jobs."""
        base = Path(".xsarena") / "jobs"
        out: List[JobV3] = []
        if not base.exists():
            return out
        for d in base.iterdir():
            p = d / "job.json"
            if p.exists():
                out.append(self.load(d.name))
        return out

    def _log_event(self, job_id: str, ev: Dict[str, Any]):
        """Log an event for a job with standardized structure."""
        ev_path = self._job_dir(job_id) / "events.jsonl"
        # Ensure standard fields are present according to schema {ts, type, job_id, chunk_idx?, bytes?, hint?, attempt?, status_code?}
        standardized_event = {
            "ts": self._ts(),
            "type": ev.get("type", "unknown"),
            "job_id": job_id,
        }
        # Add optional fields from the original event if they exist
        for key in ["chunk_idx", "bytes", "hint", "attempt", "status_code"]:
            if key in ev:
                standardized_event[key] = ev[key]

        # Add any other fields from the original event
        for key, value in ev.items():
            if key not in standardized_event:
                standardized_event[key] = value

        # Use direct file operations with flush/fsync for durability
        with open(ev_path, "a", encoding="utf-8") as e:
            e.write(json.dumps(standardized_event) + "\n")
            e.flush()
            with contextlib.suppress(Exception):
                os.fsync(e.fileno())

    @staticmethod
    def _ts() -> str:
        """Get current timestamp."""
        return datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

    def find_resumable(self, out_path_abs: str) -> Optional[str]:
        """Find an existing job (not DONE/FAILED/CANCELLED) that targets out_path."""
        target = os.path.abspath(out_path_abs)
        for job in self.list_all():
            job_out = self._normalize_out(job.run_spec)
            if job_out == target and job.state not in ("DONE", "FAILED", "CANCELLED"):
                return job.id
        return None

    def _normalize_out(self, run_spec) -> str:
        """Mirror how run spec/out_path is constructed in orchestrator/run."""
        base = (
            run_spec.out_path
            or f"./books/{run_spec.subject.replace(' ', '_')}.final.md"
        )
        return os.path.abspath(base)

```
=== END FILE: src/xsarena/core/jobs/store.py ===

=== START FILE: src/xsarena/core/config.py ===
```python
# src/xsarena/core/config.py
import os
from pathlib import Path
from typing import Any, Dict, Optional

import yaml
from dotenv import load_dotenv
from pydantic import BaseModel, ValidationError, field_validator, model_validator
from rich.console import Console

load_dotenv()

console = Console()


class Config(BaseModel):
    backend: str = "bridge"  # Default to browser-based bridge; API backends are optional for advanced use
    model: str = "default"
    window_size: int = 100
    anchor_length: int = 300
    continuation_mode: str = "anchor"
    repetition_threshold: float = 0.35
    max_retries: int = 3
    api_key: Optional[str] = os.getenv("OPENROUTER_API_KEY")
    base_url: str = "[REDACTED_URL]"  # Default to v2 bridge port
    timeout: int = 300
    redaction_enabled: bool = False

    @model_validator(mode="after")
    def validate_config(self):
        """Validate configuration values."""
        errors = []

        # Validate backend
        if self.backend not in ("bridge", "openrouter", "null"):
            errors.append(
                f"Invalid backend: {self.backend}. Valid options are: bridge, openrouter, null"
            )

        # Validate model
        if self.backend == "openrouter" and not self.api_key:
            errors.append(
                "OpenRouter backend requires api_key. Set OPENROUTER_API_KEY environment variable or configure in .xsarena/config.yml"
            )

        # Validate base_url format
        if self.base_url and not self.base_url.startswith(("[REDACTED_URL]", "[REDACTED_URL]")):
            errors.append(
                f"Invalid base_url format: {self.base_url}. Must start with [REDACTED_URL] or [REDACTED_URL]"
            )

        # Validate numeric ranges
        if self.window_size < 1 or self.window_size > 1000:
            errors.append(f"window_size must be between 1-1000, got {self.window_size}")

        if self.anchor_length < 50 or self.anchor_length > 1000:
            errors.append(
                f"anchor_length must be between 50-1000, got {self.anchor_length}"
            )

        if self.repetition_threshold < 0 or self.repetition_threshold > 1:
            errors.append(
                f"repetition_threshold must be between 0-1, got {self.repetition_threshold}"
            )

        if self.max_retries < 0 or self.max_retries > 10:
            errors.append(f"max_retries must be between 0-10, got {self.max_retries}")

        if self.timeout < 1 or self.timeout > 3600:
            errors.append(f"timeout must be between 1-3600 seconds, got {self.timeout}")

        if errors:
            raise ValueError("Configuration validation failed:\n" + "\n".join(errors))

        return self

    @field_validator("base_url")
    @classmethod
    def normalize_base_url(cls, v: str) -> str:
        """Normalize base_url to always end with /v1"""
        v = (v or "").strip()
        if not v:
            return "/v1"
        v = v.rstrip("/")
        if not v.endswith("/v1"):
            v = v + "/v1"
        return v

    def save_to_file(self, path: str) -> None:
        p = Path(path)
        p.parent.mkdir(parents=True, exist_ok=True)
        data = self.model_dump()
        p.write_text(yaml.safe_dump(data, sort_keys=False), encoding="utf-8")

    @classmethod
    def load_from_file(cls, path: str) -> "Config":
        p = Path(path)
        if not p.exists():
            return cls()
        try:
            data = yaml.safe_load(p.read_text(encoding="utf-8")) or {}
            return cls(**data)
        except ValidationError as e:
            console.print(f"[red]Validation error in config file {path}:[/red]")
            for error in e.errors():
                field = ".".join(str(loc) for loc in error["loc"])
                console.print(f"  [yellow]{field}:[/yellow] {error['msg']}")
            raise
        except Exception:
            return cls()

    @classmethod
    def load_with_layered_config(
        cls, config_file_path: Optional[str] = ".xsarena/config.yml"
    ) -> "Config":
        """Load config with layered precedence:
        defaults → .xsarena/config.yml → environment variables → CLI flags (applied by main).
        """
        # Start with defaults
        config_dict: Dict[str, Any] = {
            "backend": "bridge",  # Default to browser-based bridge; API backends are optional for advanced use
            "model": "default",
            "window_size": 100,
            "anchor_length": 300,
            "continuation_mode": "anchor",
            "repetition_threshold": 0.35,
            "max_retries": 3,
            "api_key": os.getenv("OPENROUTER_API_KEY"),
            "base_url": "[REDACTED_URL]",
            "timeout": 300,
            "redaction_enabled": False,
        }

        # Load from config file if it exists
        if config_file_path:
            config_path = Path(config_file_path)
            if config_path.exists():
                try:
                    file_config = (
                        yaml.safe_load(config_path.read_text(encoding="utf-8")) or {}
                    )
                    # Validate the file config keys against the model fields
                    unknown_keys = set(file_config.keys()) - set(
                        cls.model_fields.keys()
                    )
                    if unknown_keys:
                        console.print(
                            f"[yellow]Warning: Unknown config keys in {config_file_path}:[/yellow] {', '.join(sorted(unknown_keys))}"
                        )

                    config_dict.update(file_config)
                except Exception as e:
                    console.print(
                        f"[red]Error loading config file {config_file_path}: {e}[/red]"
                    )

        # Override with environment variables
        env_overrides = {}
        if os.getenv("XSARENA_BACKEND"):
            env_overrides["backend"] = os.getenv("XSARENA_BACKEND")
        if os.getenv("XSARENA_MODEL"):
            env_overrides["model"] = os.getenv("XSARENA_MODEL")
        if os.getenv("XSARENA_WINDOW_SIZE"):
            env_overrides["window_size"] = int(os.getenv("XSARENA_WINDOW_SIZE"))
        if os.getenv("XSARENA_ANCHOR_LENGTH"):
            env_overrides["anchor_length"] = int(os.getenv("XSARENA_ANCHOR_LENGTH"))
        if os.getenv("XSARENA_CONTINUATION_MODE"):
            env_overrides["continuation_mode"] = os.getenv("XSARENA_CONTINUATION_MODE")
        if os.getenv("XSARENA_REPETITION_THRESHOLD"):
            env_overrides["repetition_threshold"] = float(
                os.getenv("XSARENA_REPETITION_THRESHOLD")
            )
        if os.getenv("XSARENA_MAX_RETRIES"):
            env_overrides["max_retries"] = int(os.getenv("XSARENA_MAX_RETRIES"))
        if os.getenv("OPENROUTER_API_KEY"):
            env_overrides["api_key"] = os.getenv("OPENROUTER_API_KEY")
        if os.getenv("XSARENA_BASE_URL"):
            env_overrides["base_url"] = os.getenv("XSARENA_BASE_URL")
        if os.getenv("XSARENA_TIMEOUT"):
            env_overrides["timeout"] = int(os.getenv("XSARENA_TIMEOUT"))
        if os.getenv("XSARENA_REDACTION_ENABLED"):
            env_overrides["redaction_enabled"] = os.getenv(
                "XSARENA_REDACTION_ENABLED"
            ).lower() in ("true", "1", "yes")

        config_dict.update(env_overrides)

        # Create and return the validated config
        return cls(**config_dict)

    @classmethod
    def validate_config_keys(cls, config_data: Dict[str, Any]) -> Dict[str, str]:
        """Validate config keys and return unknown keys with suggestions"""
        unknown_keys = {}
        for key in config_data:
            if key not in cls.model_fields:
                # Simple suggestion: find closest matching field
                suggestions = [
                    field for field in cls.model_fields if key in field or field in key
                ]
                unknown_keys[key] = suggestions[:3]  # Return up to 3 suggestions
        return unknown_keys

```
=== END FILE: src/xsarena/core/config.py ===

=== START FILE: src/xsarena/core/state.py ===
```python
# src/xsarena/core/state.py
import json
import os
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional


@dataclass
class Message:
    role: str
    content: str
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class SessionState:
    history: List[Message] = field(default_factory=list)
    anchors: List[str] = field(default_factory=list)
    continuation_mode: str = "anchor"
    anchor_length: int = 300
    repetition_threshold: float = 0.35
    repetition_ngram: int = 4
    repetition_warn: bool = True
    backend: str = "bridge"
    model: str = "default"
    window_size: int = 100
    settings: Dict = field(default_factory=dict)
    session_mode: Optional[str] = None
    coverage_hammer_on: bool = True
    output_budget_snippet_on: bool = True
    output_push_on: bool = True
    output_min_chars: int = 3000
    output_push_max_passes: int = 3
    # New persisted toggles
    smart_min_enabled: bool = False
    outline_first_enabled: bool = False
    semantic_anchor_enabled: bool = False
    reading_overlay_on: bool = False
    # Lossless controls (optional; default off)
    lossless_enforce: bool = False
    target_density: float = 0.55
    max_adverbs_per_k: int = 15
    max_sentence_len: int = 22
    # Prompt configuration (make defaults explicit and persisted)
    active_profile: Optional[str] = None
    overlays_active: List[str] = field(default_factory=list)

    def add_message(self, role: str, content: str):
        self.history.append(Message(role=role, content=content))

    def add_anchor(self, text: str):
        self.anchors.append(text[-self.anchor_length :])

    def to_dict(self) -> dict:
        history_data = []
        for m in self.history:
            if isinstance(m, Message):
                # It's a Message object
                history_data.append(
                    {
                        "role": m.role,
                        "content": m.content,
                        "timestamp": m.timestamp.isoformat(),
                    }
                )
            elif isinstance(m, dict):
                # It's already a dict, use it as-is
                history_data.append(m)

        return {
            "history": history_data,
            "anchors": self.anchors,
            "continuation_mode": self.continuation_mode,
            "anchor_length": self.anchor_length,
            "repetition_threshold": self.repetition_threshold,
            "repetition_ngram": self.repetition_ngram,
            "repetition_warn": self.repetition_warn,
            "backend": self.backend,
            "model": self.model,
            "window_size": self.window_size,
            "settings": self.settings,
            "session_mode": self.session_mode,
            "coverage_hammer_on": self.coverage_hammer_on,
            "output_budget_snippet_on": self.output_budget_snippet_on,
            "output_push_on": self.output_push_on,
            "output_min_chars": self.output_min_chars,
            "output_push_max_passes": self.output_push_max_passes,
            "smart_min_enabled": self.smart_min_enabled,
            "outline_first_enabled": self.outline_first_enabled,
            "semantic_anchor_enabled": self.semantic_anchor_enabled,
            "reading_overlay_on": self.reading_overlay_on,
            "lossless_enforce": self.lossless_enforce,
            "target_density": self.target_density,
            "max_adverbs_per_k": self.max_adverbs_per_k,
            "max_sentence_len": self.max_sentence_len,
            "active_profile": self.active_profile,
            "overlays_active": self.overlays_active,
        }

    def save_to_file(self, filepath: str):
        with open(filepath, "w") as f:
            json.dump(self.to_dict(), f, indent=2)

    @classmethod
    def load_from_file(cls, filepath: str) -> "SessionState":
        if not os.path.exists(filepath):
            return cls()
        with open(filepath, "r") as f:
            state_dict = json.load(f)

        history = []
        for m in state_dict.get("history", []):
            if "timestamp" in m:
                timestamp = datetime.fromisoformat(m["timestamp"])
            else:
                timestamp = datetime.now()  # Default to now if no timestamp
            history.append(
                Message(
                    role=m["role"],
                    content=m["content"],
                    timestamp=timestamp,
                )
            )
        state_dict["history"] = history

        # Filter out keys that are not in the dataclass definition
        known_keys = {f.name for f in cls.__dataclass_fields__.values()}
        filtered_dict = {k: v for k, v in state_dict.items() if k in known_keys}

        return cls(**filtered_dict)

```
=== END FILE: src/xsarena/core/state.py ===

=== START FILE: src/xsarena/bridge_v2/api_server.py ===
```python
# src/xsarena/bridge_v2/api_server.py
import asyncio
import hmac
import json
import logging
import os
import sys
import time
import uuid
from contextlib import asynccontextmanager
from datetime import datetime

import uvicorn
from fastapi import FastAPI, HTTPException, Request, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse

from . import job_service as job_service_module

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Import from new modules
from .formatters import format_openai_chunk, format_openai_finish_chunk, add_content_filter_explanation
from .handlers import CONFIG, MODEL_NAME_TO_ID_MAP, MODEL_ENDPOINT_MAP, _internal_ok, load_config, load_model_map, load_model_endpoint_map, chat_completions_handler, update_available_models_handler, update_id_capture_handler
from .websocket import browser_ws, response_channels, last_activity_time, cloudflare_verified, REFRESHING_BY_REQUEST, websocket_endpoint, start_idle_restart_thread, stop_idle_restart_thread


from .payload_converter import convert_openai_to_lmarena_payload


@asynccontextmanager
async def lifespan(app: FastAPI):
    load_config()
    load_model_map()
    load_model_endpoint_map()
    start_idle_restart_thread(CONFIG)  # Start idle restart thread with CONFIG
    logger.info("Server startup complete. Waiting for userscript connection...")
    yield
    stop_idle_restart_thread()  # Stop idle restart thread
    logger.info("Server shutting down.")


app = FastAPI(lifespan=lifespan)

# Safer default CORS: localhost-only; make configurable via CONFIG
cors_origins = CONFIG.get("cors_origins") or [
    "*"
]  # Default to ["*"] when CONFIG has no cors_origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.websocket("/ws")
async def websocket_endpoint_wrapper(websocket: WebSocket):
    """Wrapper for the websocket endpoint to pass CONFIG."""
    return await websocket_endpoint(websocket, CONFIG)


@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    global last_activity_time
    # Update last activity time
    last_activity_time = datetime.now()
    
    # Call the handler from the handlers module
    return await chat_completions_handler(request, browser_ws, response_channels, REFRESHING_BY_REQUEST, cloudflare_verified)


@app.post("/internal/start_id_capture")
async def start_id_capture(request: Request):
    if not _internal_ok(request):
        raise HTTPException(status_code=401, detail="Unauthorized")
    if not browser_ws:
        raise HTTPException(status_code=503, detail="Browser client not connected.")
    await browser_ws.send_json({"command": "activate_id_capture"})
    return JSONResponse({"status": "success", "message": "Activation command sent."})


@app.post("/internal/request_model_update")
async def request_model_update(request: Request):
    if not _internal_ok(request):
        raise HTTPException(status_code=401, detail="Unauthorized")
    """Request userscript to send page source for model update."""
    if not browser_ws:
        raise HTTPException(status_code=503, detail="Browser client not connected.")
    await browser_ws.send_json({"command": "send_page_source"})
    return JSONResponse({"status": "success", "message": "Page source request sent."})


@app.post("/internal/update_available_models")
async def update_available_models(request: Request):
    if not _internal_ok(request):
        raise HTTPException(status_code=401, detail="Unauthorized")
    # Call the handler from the handlers module
    return await update_available_models_handler(request)


@app.post("/internal/id_capture/update")
async def update_id_capture(request: Request):
    if not _internal_ok(request):
        raise HTTPException(status_code=401, detail="Unauthorized")
    # Call the handler from the handlers module
    return await update_id_capture_handler(request)


# XSArena cockpit uses this to confirm IDs after capture
@app.get("/internal/config")
def internal_config(request: Request):
    if not _internal_ok(request):
        raise HTTPException(status_code=401, detail="Unauthorized")
    return {
        "bridge": {
            "session_id": CONFIG.get("session_id"),
            "message_id": CONFIG.get("message_id"),
        },
        "tavern_mode_enabled": CONFIG.get("tavern_mode_enabled", False),
        "bypass_enabled": CONFIG.get("bypass_enabled", False),
        "file_bed_enabled": CONFIG.get("file_bed_enabled", False),
        "enable_idle_restart": CONFIG.get("enable_idle_restart", False),
        "idle_restart_timeout_seconds": CONFIG.get(
            "idle_restart_timeout_seconds", 3600
        ),
        "stream_response_timeout_seconds": CONFIG.get(
            "stream_response_timeout_seconds", 360
        ),
        "api_key_set": bool(CONFIG.get("api_key")),
    }


@app.get("/v1/models")
async def list_models():
    """Return available models in OpenAI schema."""
    try:
        models_list = []
        for model_name in MODEL_NAME_TO_ID_MAP:  # Iterate over keys
            # Try to determine if it's an image model
            try:
                with open("models.json", "r", encoding="utf-8") as f:
                    models_data = json.load(f)
                model_info = models_data.get(model_name)
                if model_info and isinstance(model_info, dict):
                    if model_info.get("type") == "image":
                        pass
            except (FileNotFoundError, json.JSONDecodeError):
                pass  # If models.json doesn't exist, continue with is_image format

            model_obj = {
                "id": model_name,
                "object": "model",
                "created": int(time.time()),
                "owned_by": "user",
            }
            models_list.append(model_obj)

        return {"object": "list", "data": models_list}
    except Exception as e:
        logger.error(f"Error listing models: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/internal/reload")
def internal_reload(request: Request):
    if not _internal_ok(request):
        raise HTTPException(status_code=401, detail="Unauthorized")
    try:
        load_config()
        load_model_map()
        load_model_endpoint_map()
        return JSONResponse(
            {"ok": True, "reloaded": True, "version": CONFIG.get("version")}
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Reload failed: {e}")


# API endpoints for jobs
@app.get("/api/jobs")
async def api_list_jobs():
    """API endpoint to list all jobs."""
    job_service_instance = job_service_module.JobService()
    jobs = job_service_instance.list_jobs()

    return {"jobs": jobs}


@app.get("/api/jobs/{job_id}")
async def api_get_job(job_id: str):
    """API endpoint to get a specific job's status."""
    job_service_instance = job_service_module.JobService()
    job_data = job_service_instance.get_job(job_id)

    if job_data is None:
        raise HTTPException(status_code=404, detail="Job not found")

    return job_data


# Health endpoint expected by XSArena
@app.get("/health")
def health():
    global last_activity_time
    try:
        # Try to get last_activity_time, default to None if not defined yet
        last_activity_iso = (
            last_activity_time.isoformat() if last_activity_time else None
        )
    except AttributeError:
        last_activity_iso = None

    return {
        "status": "ok",
        "ts": datetime.now().isoformat(),
        "ws_connected": browser_ws is not None,
        "last_activity": last_activity_iso,
        "version": CONFIG.get("version", "unknown"),
    }





# Console endpoint - serves static HTML
@app.get("/console")
async def console():
    """Serve the minimal web console HTML page."""
    from pathlib import Path

    from fastapi.responses import HTMLResponse

    console_html_path = Path(__file__).parent / "static" / "console.html"
    return HTMLResponse(content=console_html_path.read_text(encoding="utf-8"))


# Alias under v1/ for some clients
@app.get("/v1/health")
def v1_health():
    return health()


def run_server():
    import os

    import uvicorn

    uvicorn.run(
        app,
        host=os.getenv("XSA_BRIDGE_HOST", "[REDACTED_IP]"),
        port=int(os.getenv("PORT", "5102")),
    )


if __name__ == "__main__":
    import os

    api_port = int(os.getenv("PORT", "5102"))
    host = os.getenv("XSA_BRIDGE_HOST", "[REDACTED_IP]")
    logger.info("🚀 LMArena Bridge v2.0 API 服务器正在启动...")
    logger.info(f"   - 监听地址: [REDACTED_URL]")
    logger.info(f"   - WebSocket 端点: ws://{host}:{api_port}/ws")
    uvicorn.run(app, host=host, port=api_port)

```
=== END FILE: src/xsarena/bridge_v2/api_server.py ===

=== START FILE: directives/_rules/rules.merged.md ===
```markdown
# CLI Agent Rules & Guidelines for XSArena Project

## Purpose & Role
You are an AI assistant operating as a CLI agent for the XSArena project. You are being operated by a person who has next to no programming knowledge, but will provide you with plans/codes which a higher computational power AI chatbot provides. You have to implement them. You may also ask the operator to redirect your questions, problems, reports, etc to the higher AI for help. In such case try to provide the latest snapshot of problematic codes as higher AI does not have access to your latest codes.

## Core Responsibilities

### 1. Project Context
- You are working with the XSArena project, a prompt studio and CLI tool for AI-assisted content creation
- Current branch is experimental with ongoing development on CLI tools, book generation, and various AI-assisted features
- The project includes CLI tools (`xsarena_cli.py`), TUI (`xsarena_tui.py`), and backend bridge components
- Key features include book generation, content rewriting, style capture/apply, and various AI-assisted workflows

### 2. Codebase Understanding
- Always check the current branch and git status before making changes
- Understand the modular architecture in `src/xsarena/` with separate modules for bridge, CLI, core, and modes
- Respect existing code conventions and patterns in the project
- Follow the existing project structure and naming conventions

## CLI Agent Operating Rules

### 3. Snapshot Command Implementation
When the command "snapshot" is given by operator, you shall:
- Output a tree structure of the project (using the `tree` command or `find`)
- Include an output of all codes in all relevant (important) files in the project
- Combine everything into a single-file txt output (snapshot.txt)
- This represents the current state of the project for higher AI troubleshooting
- Exclude binaries, CLI prompting instructions, images, downloaded modules, etc.
- Use the `xsarena ops snapshot create --mode author-core` command for consistent output (configurable via .snapshotinclude and .snapshotignore files)
- Use 'xsarena ops snapshot create --mode author-core --with-git --with-jobs' for a comprehensive debugging snapshot.
- A separate chunking script exists: `chunk_with_message.sh` which can split any file into 100KB chunks with the message "Say \"received.\" after this message. DO nothing else." appended to each chunk

### 4. File & Code Management
- Always identify and work with relevant code files (`.py`, `.sh`, `.json`, `.toml`, `.md`, `.txt`)
- Never include unnecessary files like `.git/`, `__pycache__/`, `books/`, build artifacts
- When modifying code, always maintain the existing style and patterns
- Use the `xsarena ops snapshot create --mode author-core` command to generate project snapshots (configurable via .snapshotinclude and .snapshotignore files)

### 5. Environment Cleanup
- Upon each run, check for and remove unnecessary temporary files
- Specifically look for files like `temp_*.txt`, temporary log files, or cache files
- Ask the user for permission before deleting any files they might want to keep
- Clean up any temporary files created during your operations

### 6. Error Handling & Reporting
- Document all errors encountered during operations
- Report whether you solved the issue or if it remains unresolved
- Test your solutions where possible and report the results
- If tests fail, detail what went wrong and what needs fixing

### 7. Communication & Escalation
- When encountering complex issues, suggest redirecting to the higher AI for assistance
- Provide the most recent project snapshot when requesting help from the higher AI
- Clearly explain the problem and any attempted solutions
- Include relevant code snippets and error messages

## Testing & Verification

### 8. Solution Verification
- Always test your changes to ensure they work as expected
- Run relevant tests if available
- Verify that existing functionality remains intact
- Document the testing process and results in your final reports

### 9. Final Reporting
Your final reports must be exhaustive, including:
- What happened during the operation
- What errors/problems you encountered
- How you solved them (or attempted to solve them)
- What wasn't solved or remains problematic
- Whether you tested to check that your solution worked
- What is in-waiting for future implementation
- What you want to consult/counsel with your supervisor AI about
- Any additional insights or recommendations

## Project-Specific Guidelines

### 10. Snapshot File Purpose & Content
- The snapshot file (`project_snapshot.txt`) represents the current state of the project
- It should include relevant source code files (Python, shell, config, etc.)
- It should include project directory structure information
- It excludes generated content (books/), temporary files, and external dependencies
- Its purpose is to provide context to higher AI systems for troubleshooting

### 11. Development Workflow
- Always review git status and branch before making changes
- Understand the modular architecture of `src/xsarena/`
- Follow existing patterns for CLI command implementation
- Maintain consistency with existing code style
- Respect the project's conventions for configuration and documentation

### 12. QuickRef Guidelines
- The Agent QuickRef files provide standardized workflows and settings in `directives/`:
  - `directives/agent_quickref.md` - Standard narrative approach
  - `directives/agent_quickref.compressed.md` - Compressed narrative approach
  - `directives/agent_quickref.bilingual.md` - Bilingual transformation approach
- Use these files as system text templates to ensure consistent AI behavior
- A ready-made recipe is available at `recipes/mastery.yml` for quick deployment
- These files establish consistent defaults: English-only, teach-before-use narrative, anchor continuation mode, and anti-wrap settings

### 13. Safety & Best Practices
- Never commit or modify files without user permission
- Always backup important files before modifying
- Verify your changes won't break existing functionality
- When in doubt, ask for clarification from the operator
- Document your changes for future reference

## Special Considerations

### 13. Branch Management
- The project has both `main` and `experimental` branches
- Be aware of which branch you're working on
- Understand that experimental branch may have unstable features
- Respect git workflow and don't force changes that might conflict

### 14. File Filtering for Snapshot
The snapshot should include:
- All Python source files (`*.py`)
- Configuration files (`*.json`, `*.toml`)
- Documentation files (`*.md`)
- Shell scripts (`*.sh`)
- Instruction files (`*.txt`)

The snapshot should exclude:
- `books/` directory (user-generated content)
- `__pycache__/` directories and `.pyc` files
- `.git/` directory
- `build/`, `dist/`, `node_modules/` directories
- Large binary files
- The snapshot file itself
- Temporary files

## Using QuickPaste Blocks

### 15. QuickPaste Blocks for Common Tasks
These ready-made command blocks can be pasted directly into the REPL for common operations:

**Block A — Quick Book (Bridge, after /capture)**
Replace TOPIC once. Paste the whole block.
```
/style.nobs on
/style.narrative on
/cont.mode anchor
/out.minchars 4200
/out.passes 1
/repeat.warn on
/z2h "TOPIC" --out=./books/TOPIC.final.md --max=12 --min=4200
```

**Block B — OpenRouter setup + run**
Replace TOPIC and paste.
```
/backend openrouter
/or.model openrouter/auto
/or.status
/style.nobs on
/style.narrative on
/cont.mode anchor
/out.minchars 4200
/out.passes 1
/repeat.warn on
/z2h "TOPIC" --out=./books/TOPIC.final.md --max=12 --min=4200
```

**Block C — JobSpec-first (single paste, fully repeatable)**
This is truly one block: it triggers /run.inline and includes the spec. Replace TOPIC and paste everything (including EOF).
```
/run.inline
task: book.zero2hero
subject: "TOPIC"
styles: [no-bs]
system_text: |
  English only. Teach-before-use narrative. Prose flow; avoid bullet walls.
prelude:
  - "/cont.mode anchor"
  - "/repeat.warn on"
io:
  output: file
  outPath: "./books/TOPIC.final.md"
max_chunks: 12
continuation:
  mode: anchor
  minChars: 4200
  pushPasses: 1
  repeatWarn: true
EOF
```

### 16. Helpful Macros and Tips
- **Cancel/resume anytime**: /cancel, /book.pause, /book.resume
- **If it gets listy**: /out.passes 0
- **If too short**: /out.minchars 4800; /out.passes 2
- **One-liner macro**:
  - Save: /macro.save z2h.go "/z2h \"${1}\" --out=./books/${1|slug}.final.md --max=12 --min=4200"
  - Use: /macro.run z2h.go "Your Topic"

## Final Notes
- Be creative in your approach to problem-solving
- Feel free to add or ask about anything that would improve the development process
- Always prioritize maintaining the integrity of the codebase
- When in doubt, generate a snapshot and consult with the higher AI

## Project-Keeping Rules (Added via ONE ORDER)

### Preflight for any change:
- Always run: xsarena fix run; xsarena backend ping; xsarena doctor run
- Work on a feature branch (ops/sync-<stamp> or feat/<topic>); never on main

### Cleanup (TTL + ephemeral):
- Any helper/probe must start with a header on the first line: # XSA-EPHEMERAL ttl=3d
- Preferred locations: review/ or .xsarena/tmp/ (never repo root)
- Run regular sweeps:
  - xsarena clean sweep            # dry
  - xsarena clean sweep --apply    # weekly
- Snapshot artifacts must not be committed:
  - Ignore: snapshot_chunks/, xsa_min_snapshot*.txt, review/, .xsarena/tmp/

### Content layout (enforced):
- books/finals: *.final.md, *.manual.en.md
- books/outlines: *.outline.md
- books/flashcards: *flashcards*.md
- books/archive: tiny (<64B), duplicates, obsolete
- directives/_rules/rules.merged.md is canonical; sources in directives/_rules/sources/
- directives/roles: role.*.md; directives/quickref: agent_quickref*.md; directives/prompts: prompt_*.txt

### Docs/help drift:
- If any src/xsarena/cli/*.py changes, regenerate help:
  - bash scripts/gen_docs.sh
  - If help changed, commit with: docs: update CLI help

### Snapshot discipline:
- Use only `xsarena ops snapshot create --mode author-core` command
- Default location: $HOME/xsa_min_snapshot.txt
- Do not commit snapshot outputs; delete after sending

### Jobs/run discipline:
- Prefer narrative + no_bs; avoid compressed unless explicitly chosen
- Use descriptive lengths: standard, long, very-long, max; spans: medium, long, book
- For resuming, use tail-anchor continue; only use until-end when you trust the model to emit NEXT: [END]

## Reporting Policy

### Reporting levels (use the right level for the request):
- Minimal: `xsarena report quick [--book <path>]` (default level for most requests)
- Focused: `xsarena report job <job_id> [--book <path>]` (when a specific run failed or regressed)
- Full: `xsarena report full [--book <path>]` (only when asked)

### Best practices:
- Always attach a short human summary in report.md:
  - Expected vs Actual, Command used, any manual tweaks, time/branch.
- Use quick when:
  - You need help interpreting quality/continuation issues; include the book path for a head/tail sample.
- Use job when:
  - A run failed, retried, or stalled; include the job id.
- Use full only when:
  - You're asked for recipes or directives context or a deeper dive is required.

## Adaptive Ops Rules

### Adaptive inspection and fixing
- Always run `xsarena adapt inspect` after large edits or pull/rebase; read plan in review/adapt_plan_*.json
- Only run `xsarena adapt fix --apply` on a feature branch; commit with chore(adapt): safe fixes
- If wiring warnings appear (main.py missing a command import/register), do NOT auto-patch; open an intent and ask for guidance (xsarena ops intent-new "Wire command: X")
- If help docs are missing: run scripts/gen_docs.sh; commit with docs: update CLI help
- If adapt detects risky changes or unrecognized drift, escalate:
  - `xsarena ops handoff --book <final.md>`
  - `xsarena report quick --book <final.md>`

## Reporting and Git Policy

### Reporting
- `xsarena report quick --book <final.md>` - Generate diagnostic bundle with book sample
- Snapshots only via `xsarena ops snapshot create --mode author-core` (to $HOME/xsa_min_snapshot.txt)

### Git policy
- Feature branches: feat/<topic>, fix/<topic>, chore/<topic>, ops/<topic>
- Conventional commits: feat:, fix:, chore:, docs:, refactor:, test:, build:, ci:
- Run `scripts/prepush_check.sh` before push (lint/format/tests/help drift; no ephemeral in diff)

### Adapt learning
- `xsarena adapt suppress-add <check> [--pattern "..."]` - Suppress expected/benign warnings
- `xsarena adapt suppress-ls` - List current suppressions
- `xsarena adapt suppress-clear <check>|all` - Clear suppressions
- Suppressions stored in `.xsarena/ops/pointers.json`

## Memory Policy

### ONE ORDER handling
- After I paste a ONE ORDER, save it to review/one_order_<ts>.md and append to directives/_rules/sources/ORDERS_LOG.md, then run: bash scripts/merge_session_rules.sh

## Snapshot Policy

### Size constraint
- Snapshot files must be within 300-400KB range
- Use `tools/minimal_snapshot_optimized.py` for size-optimized snapshots
- If snapshot exceeds 400KB, review and limit included files

### Anti-recursion check
- After creating a snapshot, verify it doesn't include previous snapshots in the output
- Check snapshot content for recursive inclusion of snapshot files
- Look for patterns like xsa_min_snapshot*.txt or similar in the output tree/file list

## Low AI Reliability Considerations

### Context and Instruction Issues
- Lower AI is unreliable and sometimes available when context runs out
- Instructions from lower AI may include problems or contradictions
- Always verify implementation completeness using `xsarena checklist status`
- When lower AI gives instructions, cross-reference with established patterns
- If lower AI instructions conflict with working implementations, prioritize working code
- Use `docs/IMPLEMENTATION_CHECKLIST.md` as authoritative reference for completed work

---

# Orders Log (append-only)
# Append "ONE ORDER" blocks here after each major instruction.

# ONE ORDER: Communication Procedures for Higher AI
- Save "Communication Rules for Higher AI" into docs/HIGHER_AI_COMM_PROTOCOL.md
- Re-merge rules so the canonical file includes CLI agent rules
- Generate a "missing-from-assistant" snapshot that lists and inlines contents of files not seen yet
- Confirm rules coverage with: fgrep -n "CLI Agent Rules" directives/_rules/rules.merged.md
- Tasks completed: 1) Created docs/HIGHER_AI_COMM_PROTOCOL.md, 2) Verified merge script includes CLI agent rules, 3) Generated missing files snapshot at review/missing_from_assistant_snapshot.txt, 4) Confirmed CLI Agent Rules in merged file
# ONE ORDER — Pre‑Snapshot Cleanup Policy (project root + chunks dir + home)
Date (UTC): 2025-10-14 23:43:11
Intent:
- Before any snapshot or situation report, remove stale snapshot outputs to avoid drift or duplication.
- Clean only:
  - Project root (top-level files): situation_report.*.txt/health/part*, xsa_snapshot_pro*.txt(.tar.gz), xsa_min_snapshot*.txt, xsa_final_snapshot*.txt, xsa_final_cleanup_snapshot*.txt
  - Chunks dir: snapshot_chunks/ (files inside; remove dir if empty)
  - Home (~, top-level files only): xsa_min_snapshot*.txt, xsa_snapshot_pro*.txt(.tar.gz), situation_report.*.txt/part*
- Do not touch subdirectories of ~ or other project subdirectories (review/, docs/, .xsarena/).

Notes:
- This order is additive and must run first in any snapshot/situation-report workflow.
- Redaction/snapshotting code remains unchanged by this order.


# ONE ORDER — Snapshot Healthcheck and Cleanup Policy
Date (UTC): 2025-10-15 20:52:00
Intent:
- Before running any snapshot utility, clean existing snapshot outputs to prevent stale/included data
- Include project source, configuration, and documentation; exclude generated content like books/finals
- Verify snapshot contains required sections and has reasonable size
- Maintain snapshot hygiene through automated healthchecks

Specific Requirements:
1. Clean existing snapshots: remove all snapshot_*.txt files from .xsarena/snapshots/ and project root
2. Include: src/, directives/, recipes/, scripts/, docs/, config files, rules, tools/
3. Exclude: books/finals/, books/outlines/, other generated output content
4. Verify: directory trees, health checks, and footer are present
5. Check: size should be between 50KB-500KB (not too small, not including massive outputs)

Implementation:
- Run cleanup before each snapshot operation
- Use `xsarena snapshot write --dry-run` for automated verification
- Follow inclusion/exclusion patterns in tools/snapshot_txt.py
- Maintain reasonable chunk sizes (default 120KB, max ~400KB per chunk)

Rationale:
- Prevents inclusion of stale snapshot outputs in new snapshots
- Keeps snapshots focused on project state rather than generated content
- Ensures snapshot utility reliability and consistency
- Maintains appropriate snapshot sizes for processing and sharing

# ONE ORDER — Cockpit Prompt Commands
- Add /prompt.show, /prompt.style <on|off> <name>, /prompt.profile <name>, /prompt.list, /prompt.preview <recipe>.
- Runs must honor selected overlays and profile (compose overlays + extra into system_text).
- Persist overlays_active and active_profile in .xsarena/session_state.json settings.
- Default overlays: narrative + no_bs when none are set.

```
=== END FILE: directives/_rules/rules.merged.md ===

=== START FILE: directives/base/zero2hero.md ===
```markdown
SUBJECT: {subject}

ROLE
You are a seasoned practitioner and teacher in {subject}. Write a comprehensive, high‑density self‑study manual that takes a serious learner from foundations to a master's‑level grasp and practice.

COVERAGE CONTRACT (do not violate)
- Scope: cover the entire field and its major subfields, theory → methods → applications → pitfalls → practice. Include core debates, default choices (and when to deviate), and limits of claims.
- Depth: build from zero to graduate‑level competence; teach skills, not trivia. Show decisive heuristics, procedures, and failure modes at the point of use.
- No early wrap‑up: do not conclude, summarize, or end before the whole field and subfields are covered to the target depth. Treat "continue." as proceeding exactly where you left off on the next input.
- Continuity: pick up exactly where the last chunk stopped; no re‑introductions; no throat‑clearing.

VOICE AND STANCE
- Plain, direct Chomsky‑style clarity. Simple language; expose assumptions; no fluff.
- Be decisive when evidence is clear; label uncertainty crisply. Steelman competing views, then choose a default and reason.

STYLE
- Mostly tight paragraph prose. Use bullets only when a read-and-do list is clearer.
- Examples only when they materially clarify a decision or distinction.
- Keep numbers when they guide choices; avoid derivations.

JARGON
- Prefer plain language; on first use, write the full term with a short parenthetical gloss; minimize acronyms.

CONTROVERSIES
- Cover directly. Label strength: [robust] [mixed] [contested]. Present main views; state when each might be right; pick a default and give the reason.

EVIDENCE AND CREDITS
- Name only canonical figures, laws, or must‑know sources when attribution clarifies.

PRACTICALITY
- Weave procedures, defaults/ranges, quick checks, and common failure modes where they matter.
- Include checklists, rubrics, and projects/exercises across the arc.

CONTINUATION & CHUNKING
- Write ~800–1,200 words per chunk; stop at a natural break.
- End every chunk with one line: NEXT: [what comes next] (the next specific subtopic).
- On input continue. resume exactly where you left off, with no repetition or re‑introductions, and end again with NEXT: [...]
- Do not end until the manual is complete. When truly complete, end with: NEXT: [END].

BEGIN
Start now from the foundations upward. No preface or meta; go straight into teaching.

```
=== END FILE: directives/base/zero2hero.md ===

=== START FILE: directives/system/plan_from_seeds.md ===
```markdown
# directives/system/plan_from_seeds.md
You are an editorial planner for a long-form self-study manual.
The user will provide rough seeds (topics/notes). Your job:
- subject: one-line final title (concise, specific)
- goal: 3–5 sentences (scope, depth, audience, exclusions)
- focus: 5–8 bullets (what to emphasize/avoid)
- outline: 10–16 top-level sections; each item has:
    - title: short section heading
    - cover: 2–4 bullets of what to cover
Return STRICT YAML only with keys: subject, goal, focus, outline. No code fences, no commentary.

Seeds:
<<<SEEDS
{seeds}
SEEDS>>>

```
=== END FILE: directives/system/plan_from_seeds.md ===

