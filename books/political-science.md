Politics is collective decision-making under scarcity and disagreement. Political science explains how rules get made, enforced, and changed; who benefits and who loses; and why patterns persist or break. Start by fixing three pillars: power (who can get compliance), institutions (the rules of the game), and preferences/ideas (what actors want and how they see the world). Every analysis you do should make each pillar explicit.

Power. Three useful lenses:
- Dahl’s “first face” of power: A gets B to do what B would not otherwise do. [robust] It fits observable conflicts, votes, and coercion.
- Bachrach–Baratz “second face”: A keeps some issues off the agenda. [mixed] Best for settings with institutional chokepoints and agenda control (committees, courts).
- Lukes’ “third face”: A shapes B’s preferences so B consents. [contested] Use carefully; you need evidence of internalized norms or propaganda effects, not just outcomes you dislike.

Default: Start with the first face. Add the second when agenda control is institutionalized. Use the third only with direct evidence of preference formation (education, media exposure) and clear counterfactuals.

The state. Max Weber’s definition—an organization that claims a monopoly on the legitimate use of force within a territory—is still the cleanest. In analysis, separate capacity (ability to implement decisions), legitimacy (belief in right to rule), and autonomy (freedom from capture by private interests). [robust] Most state effects run through capacity (tax, administer, coerce). Legitimacy lowers enforcement costs. Autonomy conditions long-run development.

Legitimacy. Empirical legitimacy (measured by survey trust and compliance) and moral legitimacy (normative justification) are distinct. Keep them separate: you can empirically show obedience without endorsing it. Common failure mode: smuggling moral claims into empirical inference.

Collective action. Public goods create free-riding. Olson’s logic: large groups struggle; small groups with selective incentives or strong social ties do better. [robust] Defaults:
- To get provision, add selective benefits/costs (fees, club goods, sanctions), increase iteration (repeated interaction), or shrink the effective group (federalism, delegation).
- Use reputations and monitoring to reduce moral hazard; use screening to reduce adverse selection.

Coordination vs cooperation. Coordination problems have multiple good equilibria; no one wants to defect once an equilibrium is chosen. Cooperation problems have temptations to defect even after agreement. Schelling focal points, precedents, and bright-line rules solve coordination. Enforcement and credible commitment solve cooperation. Quick check: if everyone prefers “same-side” choices (drive left or right), it’s coordination; if each prefers unilateral deviation (tariff cuts, disarmament), it’s cooperation.

Institutions. Institutions are formal and informal rules that structure incentives. They:
- Allocate agenda power and vetoes (e.g., committees, courts).
- Shape information flows (transparency, secrecy).
- Set enforcement and dispute resolution.
- Persist via increasing returns and expectations (path dependence). [robust]

Default: model outcomes as equilibrium behavior under given rules and beliefs. Deviate when rules are not enforced (weak capacity) or when norms overpower formal rules (tight-knit communities; Ostrom-style self-governance).

Ideas and identity. Beliefs, frames, and social identities shape preferences and perceived interests. [mixed] When material incentives are ambiguous or distant, ideas matter more; when incentives are sharp and immediate (jobs, taxes), they dominate. Don’t assume “false consciousness.” Instead, trace how information, elite cues, and peer networks update beliefs.

Information. Politics is riddled with asymmetric information. In elections: voters infer quality and ideology from signals (party, endorsements, performance). In international relations: states bluff or signal via costly actions. Defaults:
- Expect signaling through costly, observable moves.
- Expect misinformation where verification is hard and identity-linked beliefs are strong.

Credible commitment. Many political failures are not about preference conflict but inability to credibly promise future behavior (property rights under weak rule of law; disarmament). Solutions: third-party enforcement (courts), tying hands (constitutional limits), monitoring plus sanctions (trade institutions), or self-enforcing equilibria (reputation in repeated games). [robust]

Principal–agent problems. Voters–politicians, politicians–bureaucrats, civilians–military. Hazards: hidden action (shirking), hidden information (selection). Tools: screening (primaries, exams), monitoring (audits, media), incentives (pay, promotion), and discretion (letting experts adapt). [robust] Default: combine modest incentives with targeted monitoring; avoid over-specification that destroys expertise.

Levels of analysis. Individual (voters, leaders), organizational (parties, agencies, firms), institutional (rules), and system (regimes, international order). Cross-level causation is common. Default workflow: explain micro-behavior with incentives and identities; aggregate via institutions; check system-level feedbacks (conflict, diffusion).

Time. Many political forces are slow-moving (demography, party coalitions); others are fast shocks (crises, scandals). Distinguish short-run equilibrium analysis from long-run dynamic change. Expect hysteresis: once coalitions and rules lock in, reversal is costly.

Concepts and measurement. Define concepts with minimal criteria and clear boundaries (Sartori). Avoid “conceptual stretching” (calling any NGO a “state” or any unrest a “revolution”). Operationalize with transparent coding rules. If a concept is multidimensional (democracy), either disaggregate (contestations, participation) or justify a composite index.

Subfields and core questions (map for later sections):
- Political theory: what should justify authority, rights, and justice? Tools: argument analysis; tradition mapping.
- Comparative politics: why regimes and policies vary across countries and time?
- International relations: why war, peace, and cooperation among states and non-state actors?
- Public policy/administration: how governments design and implement programs?
- Political behavior: how citizens form preferences, participate, and vote?
- Political economy: how politics and markets co-produce outcomes?
- Methods: research design, qualitative and quantitative inference, formal modeling, experiments.

Default modeling stance. Start with strategic, resource-constrained actors with bounded rationality under incomplete information. Allow stable preferences within a context, but permit identity-linked utility where evidence supports it. Incorporate norms when enforcement or reputation is observable. Deviate from strategy when habits or emotions clearly dominate (disaster voting swings, panic). Label your scope conditions.

Quick diagnostic checklist (use for any puzzle):
- Outcome: what exactly varies? where? when?
- Actors: who matters? who is organized?
- Preferences: material, status, identity? how intense?
- Information: who knows what? signals?
- Rules: agenda power, vetoes, enforcement, exit options
- Capacity: who can implement or block?
- Time: one-shot or repeated? discounting?
- Outside options: alternatives to compliance?
- Mechanisms: coordination, cooperation, coercion, exchange, learning, selection
- Scope: what cases fit these conditions?

Common failure modes:
- Treating institutions as intentions (“constitution says X, so X happens”).
- Ignoring selection (people self-select into protests; leaders into war).
- Assuming one level explains all (micro-only or structure-only accounts).
- Conceptual stretching and index worship without construct validity.
- Moralizing empirical claims (confusing “is” with “ought”).

Practice project (1–2 hours):
- Pick a recent policy change (e.g., minimum wage increase). Use the checklist to map actors, preferences, rules, and information. Identify the main mechanism (coordination? credible commitment? interest group pressure?). Write a 1-page memo predicting two knock-on effects and one plausible counter-move by losers.

Practice project (longer, 1–2 days):
- Case comparison: select two similar cities or countries with different outcomes on a salient issue (policing style, pandemic policy). Specify the outcome, argue which confounders you hold constant, and propose at least two mechanisms that differ. Sketch evidence you would collect to adjudicate (votes, budgets, interviews, archival minutes, media).

Heuristic rules of thumb:
- If preferences are diffuse and benefits concentrated, expect organized minorities to win unless transparency and turnout are high. [robust]
- If enforcement is cheap and monitoring is easy, rules will bite; otherwise, expect drift. [robust]
- Fewer veto players → faster policy change but risk of swings; more veto players → stability but gridlock (Tsebelis). [robust]
- Crises shift power to executives; oversight often lags and rarely fully recovers. [mixed]

These foundations are the backbone. Next we build the tools to make claims clean and test them.

Start with causal questions. Write them in a tight sentence: what is the effect of X (the cause you can, in principle, manipulate) on Y (the outcome) for units U over time T in setting S? Name the estimand (the quantity you seek): average treatment effect, effect of treatment on the treated, or a contrast across policies. If you can’t state the estimand, you’re not ready to design a study.

Use the potential outcomes frame (what would happen with and without X for the same unit) to anchor thinking. [robust] Use directed acyclic graphs (simple causal diagrams) to list confounders (variables that cause both X and Y), mediators (causal pathway from X to Y), and colliders (common effects of two variables). [robust] Defaults:
- Block backdoor paths (confounding) by design (randomization) when possible; otherwise by conditioning on true confounders.
- Do not control for mediators if you want the total effect.
- Never condition on colliders or their descendants; it opens spurious paths.

Internal vs external validity. Internal validity asks “is the estimated effect causal for this sample under these assumptions?” External validity asks “does it generalize to other places, people, and times?” Default: maximize internal validity first. A clean local effect beats a messy general claim. Generalize by theory: identify which mechanisms travel and which scope conditions matter; use reweighting or replication across contexts to check transportability. [robust]

Design hierarchy (not moral, just diagnostic). Randomized experiments (true random assignment) are strongest on internal validity. Natural experiments (“as-if random” assignment) can be strong if the assignment process is well understood. Observational designs rely on assumptions; they can be persuasive when assumptions are credible and tested. Defaults:
- If you can randomize ethically and feasibly, do it.
- If randomization is impossible, look for discontinuities, lotteries, thresholds, or exogenous shocks.
- If neither exists, build a comparative design that credibly rules out confounding and selection.

Core identification strategies and when to use them:
- Randomized experiments: lab, survey, field. Use when you control assignment. Analyze with difference-in-means; add covariates for precision, not identification. Cluster standard errors at the assignment level. Plan sample size with power analysis; underpowered studies waste information. [robust]
- Regression discontinuity (sharp or fuzzy): when treatment is assigned by a threshold rule. Estimate local effects near the cutoff; choose bandwidth with data-driven methods; report sensitivity to bandwidth and polynomial order; show no sorting near cutoff. [robust]
- Difference-in-differences: when treated and control groups have parallel trends in absence of treatment. Test pre-trends visually and with event-study models; account for staggered adoption using modern estimators that avoid negative weighting; cluster at unit or higher. [mixed] Default: use event-study with group-time interactions; report cohort-specific effects.
- Instrumental variables: when you have a cause of X that affects Y only through X and is as good as random. Test for relevance (strong first stage); argue exclusion with substance and falsification tests (pleiad of controls, overidentification sparingly). Weak instruments bias you; use limited information maximum likelihood or Anderson–Rubin when weak. [mixed]
- Matching/weighting: balance observed covariates between treated and control. Use it as a design step pre-analysis; don’t trust balance on unobservables. Combine with difference-in-differences or instrumental variables when possible. [mixed]
- Synthetic control: for one or few treated units over time. Build a weighted combination of controls to mirror pre-treatment trajectories; inspect fit; conduct placebo and leave-one-out checks. [robust for big shocks with good panels]

Measurement. Concept clarity first. Define minimal attributes; decide if the concept is unidimensional. If multidimensional (e.g., democracy), either analyze components separately or justify an index with transparent weights. Guard against:
- Non-differential measurement error (attenuates effects).
- Differential error (biases effects; common in self-reports).
- Invariance violations across groups/time (the same survey item may mean different things). Quick check: test item response models; use anchoring vignettes sparingly. Default: triangulate using multiple indicators (surveys, administrative data, text).

Sampling and data. Specify the target population and sampling frame. If using convenience or online panels, state limits; use quotas and post-stratification to adjust, but do not oversell representativeness. Track missing data; assume not missing at random until you have reason otherwise; use multiple imputation with sensitivity analysis. Clean data rigorously: deduplicate, verify ranges, and document decisions.

Analysis defaults (for most designs):
- Start with design-based estimators; keep models simple and interpretable.
- Report effect sizes with uncertainty: confidence intervals or, better, posterior intervals if using Bayesian methods. Avoid sole reliance on p-values.
- Cluster standard errors at the level of treatment or shock; if clusters are few (<50), use wild cluster bootstrap.
- Show robustness: alternative specifications that do not change identification, leave-one-out, and placebo tests aligned with your design.

Pre-analysis and multiplicity. Distinguish exploration (learning patterns) from confirmation (testing pre-specified claims). When feasible, pre-register hypotheses, outcomes, and analysis plans. If you test many outcomes or subgroups, control false discoveries (Benjamini–Hochberg for rate control; Holm for familywise error). Better: declare a primary outcome and a small set of key moderators; treat the rest as exploratory.

Heterogeneous effects. Theory should predict where effects differ. Interact treatment with pre-treatment covariates; avoid fishing. For many moderators, use honest machine learning (sample splitting, cross-fitting) to discover heterogeneity; reserve a holdout for confirmation. [mixed] Report conditional average treatment effects with uncertainty and clear subgroup definitions.

Assumption checks and sensitivity. You will never prove assumptions; you can probe them.
- For unobserved confounding, use sensitivity tools (Rosenbaum bounds, Oster’s delta, partial R2-based tipping points) to ask how strong an omitted variable must be to overturn results.
- For difference-in-differences, show event-study coefficients with narrow confidence bands pre-treatment; test for anticipation.
- For instrumental variables, use balance checks on instrument; test for monotonicity plausibility in context.
- For regression discontinuity, show density test at cutoff (no manipulation) and covariate balance locally.

Common failure modes at point of use:
- Controlling for post-treatment variables (mediators) when estimating total effects; this biases toward zero or flips signs.
- Conditioning on colliders (e.g., analyzing only conflict that escalated to war) creating selection bias.
- Bad instruments (policy preference as an instrument for policy) where exclusion is implausible.
- Parallel-trends hand-waving without pre-trend evidence.
- Overfitting with rich models and reporting in-sample fit as causal evidence.
- Ignoring interference (one unit’s treatment affects another). When spillovers exist, redesign: cluster-level treatment, measure exposure, or use network designs.

Ethics and risk. Political research can harm subjects: reputational, legal, economic, or physical. Principles: minimize risk, maximize value, respect autonomy. Get informed consent unless justified deception is essential and low risk; debrief where possible. In fragile contexts, do a harm assessment: could participation expose someone to retaliation? Protect data with encryption and access controls; strip identifiers; pre-plan destruction timelines. Coordinate with institutional review boards; but don’t outsource judgment to them.

Documentation and reproducibility. Share replication materials: raw and cleaned data, code, and a README that lets others rebuild tables and figures in one run. Label variables clearly; track versions. Default to open science unless privacy or agreements prohibit; then provide synthetic or redacted data with a clear explanation.

A minimal workflow you can reuse:
- Write the causal question and estimand.
- Draw a diagram; list confounders, mediators, colliders.
- Pick the cleanest feasible design; commit in writing.
- Plan measurement and sampling; pilot instruments.
- Power analysis; recruit accordingly.
- Pre-register key hypotheses when appropriate.
- Lock the analysis code skeleton before seeing outcomes.
- Execute; monitor for protocol deviations.
- Analyze with design-first estimators; show assumption checks.
- Report results with effect sizes, uncertainty, and limits; release materials.

Practice exercises:
- Redraw three recent causal claims in news or papers as diagrams; identify at least one backdoor path and propose a design to block it.
- Take a public dataset on a policy change; conduct a pre-trend and event-study analysis; write two paragraphs on whether parallel trends hold and why.

Default reporting standard:
- State the question and estimand.
- Describe the design and why assumptions are plausible in this context.
- Show the main effect with uncertainty and an interpretable scale.
- Provide design-aligned diagnostics and sensitivity.
- Discuss scope conditions and external validity with specific contrasts.

Qualitative inference complements and tests causal claims when variables are hard to measure, events are rare, and mechanisms matter. Default uses: concept formation, scope condition mapping, mechanism testing inside cases, and designing better quantitative studies.

Case selection. Avoid selecting on the outcome alone; it overstates fit and hides counterfactuals. Prefer theory-guided variation: include positives and negatives for the mechanism or condition you care about. Two comparative designs:
- Most-similar systems: hold many background factors constant; vary the putative cause. Good for isolating one factor, but hidden differences can still confound.
- Most-different systems: vary many backgrounds; find a common cause with the common outcome. Stronger when the shared cause is rare. [mixed]
Default: pair one “treated” and one “untreated” case that are close on obvious confounders; add a negative case to test necessity.

Process tracing (within-case causal testing). Treat each hypothesized mechanism as a sequence of events that would leave distinctive traces. Use diagnostic tests with Bayesian logic:
- Straw-in-the-wind: weakly increases belief if present; absence is not fatal.
- Hoop test: must be passed to keep the hypothesis alive; failing it is fatal.
- Smoking gun: presence is near-decisive; absence is not fatal.
- Doubly decisive: confirms one hypothesis and rules out rivals. [robust]
Default workflow:
- Specify alternative mechanisms up front (minimum of two, ideally three).
- Derive predicted evidence patterns (documents, timing, statements, budget lines) for each.
- Build a precise timeline; look for sequencing and lags consistent with the mechanism.
- Seek negative evidence (what should not exist if the mechanism holds).
- Update beliefs explicitly: note which tests moved your posterior and why.
Failure modes: survivorship and archival bias (missing records), naive credence in elite statements, and post hoc mechanism invention. Triangulate across sources and incentives.

Comparative historical analysis. Use when sequences and critical junctures shape institutions (state formation, welfare regimes). Identify junctures (moments where multiple futures were plausible), reactive sequences (chains triggered by initial events), and path dependence (increasing returns lock-in). Evidence: contemporaneous debates showing live alternatives; early small differences magnified over time. [mixed] Default: write explicit counterfactual branches at each juncture and list what observations would distinguish them. Avoid “just-so” narratives.

Set-theoretic methods (qualitative comparative analysis). Model outcomes as configurations of necessary and/or sufficient conditions, with crisp or fuzzy membership. Useful when you expect conjunctural causation and equifinality with small N. [contested] Pitfalls: calibration subjectivity, limited diversity, sensitivity to coding, and logical remainders. Default: use only when theory predicts specific configurations; run robustness (vary thresholds, random relabeling); report necessity/sufficiency with consistency and coverage; avoid causal claims without external validation.

Interviews and fieldwork. Sample purposively for variation in role and incentive; snowball to reach hidden actors, but cap chain-length to avoid echo chambers. Write neutral, open questions; avoid leading prompts; probe for specific instances and documents. Record when safe and consented; otherwise take structured notes. Cross-check claims across actors with different incentives. Manage power and safety: assess risk to respondents; anonymize when retaliation is plausible; secure data. [mixed] Default: pair interviews with document review; code claims by type (fact, motive, evaluation); flag contradictions for follow-up.

Ethnography and observation. Use when practices and norms are opaque to outsiders and when behavior diverges from stated rules. Gain access ethically; be clear about your role. Keep a field log separating observation from interpretation. [mixed] Risk: over-immersion and loss of critical distance; mitigate by periodic memos tying observations back to theory and rival explanations.

Archival work. Before entering an archive, draft your mechanism tests and a list of “smoking gun” and “hoop” items (memos, minutes, budget lines). Track provenance and gaps; archives are curated, not neutral. Photograph and index; maintain a citation database. Default: replicate claims with at least two independent series (e.g., party files and ministry files).

Text and content analysis. Manual coding yields nuance but scales poorly; automated methods scale but need validation.
- Dictionaries: transparent but brittle across domains; pretest and hand-tune; update sparingly; show human–machine agreement.
- Supervised classification: label a stratified sample; hold out a test set; report precision/recall; beware label leakage (metadata).
- Topic models (e.g., latent Dirichlet allocation): exploratory; do not treat topics as measures without exogenous validation; rotate and label with care.
- Scaling (e.g., Wordfish, ideal points from text): require assumptions about word usage stability; validate against known positions. [mixed]
Default: treat text outputs as noisy measures; triangulate with human coding, external events, and predictive checks.

Surveys and experiments in surveys. Wording, order, and response formats shape answers. Defaults:
- Keep questions concrete; avoid double-barreled items; randomize order within blocks.
- Use balanced scales with clear labels; avoid agree/disagree when acquiescence bias is high.
- Combat social desirability: list experiments for sensitive prevalence; endorsements or vignettes for treatment effects; randomized response for high-risk admissions.
- Conjoint experiments: good for multi-attribute choices; report marginal means and AMCEs; randomize profiles fully or use strategy-proof designs; caution on transportability to real-world choices. [mixed]
Weighting and post-stratification adjust samples; report design effects; check stability across weighting schemes.

Networks and interference. Political processes are networked (patronage, movements, diffusion). SUTVA (no interference) often fails. Designs:
- Cluster randomization at natural groups; estimate direct and spillover effects.
- Saturation designs: vary treatment density across clusters; model exposure.
- Observational diffusion: use event history with exposure risk sets; instrument exposure if possible (geography, logistics shocks); beware homophily. [mixed]
Default: define exposure mappings ex ante; report sensitivity to alternative network definitions.

Geospatial analysis. Spatial units and boundaries can induce bias (modifiable areal unit problem). Defaults:
- Visualize; test Moran’s I (spatial autocorrelation); if present, cluster or model spatial dependence.
- Geographic regression discontinuity: compare units just across borders; demonstrate continuity of covariates; use narrow bandwidths.
- Spatial difference-in-differences: allow spatially varying trends; include border-by-time trends where appropriate. [mixed]
Avoid overinterpreting night lights and mobility proxies without calibration to ground outcomes.

Machine learning for description and prediction. Use ML to classify, extract features, and predict outcomes; do not confuse prediction with causation. Defaults:
- Split data: training/validation/test; prevent leakage.
- Pick simple models first; escalate only if gains justify complexity.
- For causal estimation with high-dimensional controls, use double/debiased machine learning; cross-fit; report nuisance model diagnostics.
- For heterogeneous treatment effects, use causal forests or T-learners with honest splitting; confirm on a holdout or new data. [mixed]
Beware fairness issues: models can encode and amplify bias; assess subgroup performance and error asymmetries.

Mixed methods. Sequence designs so that each method tests what the other cannot. Two productive patterns:
- Quant → Qual: use a broad estimate to locate anomalies; trace mechanisms in outliers to refine theory.
- Qual → Quant: develop concepts and mechanisms; encode them and test transportability at scale.
Default: pre-specify how each part can falsify the other; avoid “illustrative” qualitative appendices that add no test.

Practical exercise (half day):
- Pick a published quantitative paper in your area. Write two rival mechanisms that could generate its main effect. Design a process-tracing plan: list at least three diagnostic pieces of evidence per mechanism, their likely sources, and which type of test they are. Conduct one archival or media search and report whether you found one predicted trace.

Formal models clarify incentives and strategic interaction. Use them to derive crisp comparative statics and to see where intuition fails. Treat models as maps: simplified, explicit, and wrong in details but useful if they isolate the mechanism you care about.

Build a model in five moves:
- Define players, timing, and information. Who moves when? What does each know at each node? Default: start with two players; add more only if necessary.
- Specify actions and constraints. Keep action sets minimal; include outside options and exit if they’re salient.
- Write payoffs as utilities that capture trade-offs. Normalize where convenient; encode uncertainty explicitly. Default: linear payoffs unless curvature matters (risk, diminishing returns).
- Solve for equilibrium appropriate to information and timing. Static, complete information: Nash equilibrium. Dynamic, complete: subgame perfect equilibrium (solve by backward induction). Incomplete information: perfect Bayesian or sequential equilibrium. [robust]
- Derive testable implications: how do equilibrium actions change when parameters change (costs, probabilities, veto players)? What patterns should we observe?

Refinements and selection. Many games have multiple equilibria. Use refinements when they rest on observable discipline:
- Subgame perfection eliminates non-credible threats. [robust]
- Trembling-hand perfection and forward induction remove equilibria propped by zero-probability moves; use sparingly. [mixed]
- Intuitive criterion (for signaling) removes equilibria relying on implausible off-path beliefs. [mixed]
- Global games select risk-dominant equilibria when actors have tiny payoff noise (currency attacks); powerful but narrow scope. [mixed]
Default: report all plausible equilibria; pick a focal equilibrium with a behaviorally grounded selection story (precedent, focal points, norms), and state the scope conditions.

Canonical games you should master and reuse:
- Collective action/public goods. N-person prisoner's dilemma; free-riding is equilibrium unless selective incentives, repeated interaction, or small groups. Mechanisms: trigger strategies in repeated play; club goods; social sanctions. Comparative static: provision rises with stronger monitoring and higher discount factors. [robust]
- Coordination (stag hunt). Two equilibria: safe/low and risky/high. Policy relevance: bank runs, institutional reform. Coordination devices: focal points, guarantees, credible signals. [robust]
- Bargaining. Nash bargaining solution for cooperative insights; Rubinstein alternating-offers for dynamic noncooperative bargaining. Comparative statics: proposer advantage rises with impatience of the other side; breakdown risk pulls agreements toward myopic preferences. Legislative bargaining (Baron–Ferejohn): agenda power yields formateur premia; larger discounting speeds deals but increases inequality in shares. [robust]
- Signaling and screening. Sender knows type; receiver updates beliefs. Separating vs pooling equilibria depend on cost differentials and prior beliefs. Applications: candidate quality signals (education, experience), crisis diplomacy (costly audience costs claims [contested]), insurgent strength signals (attacks). Key checks: single-crossing, off-path beliefs, and whether costs are truly type-dependent.
- Principal–agent. Incentive contracts under hidden action; screening under hidden information. Tools: participation and incentive compatibility constraints; use limited liability if real. Applications: bureaucratic performance pay, military rules of engagement, regulatory oversight. Comparative statics: stronger monitoring reduces moral hazard but may crowd out expertise. [robust]
- Contests (Tullock). Actors expend effort to win prizes; success is probabilistic. Applications: lobbying, patronage competition, repression–resistance. Comparative statics: more players can increase waste; larger prizes increase effort and dissipation. [mixed]
- Conflict and war bargaining. Fearon’s logic: private information with incentives to misrepresent, commitment problems, and indivisibilities explain war despite inefficiency. Predictions: wars are rare; mobilization and tying hands can signal but also lock leaders in. Policy levers: third-party guarantees to relax commitment problems; transparency to reduce private info. [robust core; some audience-cost mechanisms contested]

From model to empirical work. Two routes:
- Reduced-form implications: deduce directional predictions (who gets agenda power does better; higher monitoring reduces shirking). Test with designs from earlier. Pros: fewer parametric assumptions. Cons: weaker mapping to primitives.
- Structural estimation: specify functional forms; estimate parameters to fit behavior (e.g., ideal points and proposal power to match roll calls). Pros: counterfactuals and policy simulations; interpretability. Cons: strong assumptions; misspecification risk. Default: start reduced-form to establish existence; use structural when policy design needs magnitudes and welfare comparisons. Validate structure with out-of-sample tests and behavioral moments you did not target.

How to keep models disciplined:
- Start with the simplest model that captures the core tension; add complexity only to address a failure of fit or a new question.
- Tie at least one parameter to observable quantities (discounting to term lengths; monitoring probability to audit rates).
- Check robustness to small perturbations (payoffs, tie-breaking). If predictions flip with tiny changes, your mechanism is fragile.
- Map each variable to a measurement plan. If you cannot measure a key parameter or implication, reconsider the model or treat it as exploratory theory.

Common failure modes:
- Modeling preferences that smuggle outcomes (assuming elites love inequality to explain inequality). Keep preferences thin; put weight on constraints and incentives unless identity is the object of study.
- Using equilibrium that requires heroic coordination when institutions and history favor another. State the selection mechanism.
- Ignoring outside options and exit; many political failures vanish when exit is real (capital flight, emigration).
- Calibrating to fit one fact and ignoring others the model also implies. Build a fact deck; test multiple implications.

Mechanism design basics for institutions. Often we want rules that implement desired outcomes under private information and strategic behavior.
- Revelation principle: if something is implementable, there is a direct mechanism where agents truthfully report types under incentive compatibility. Use it to simplify design. [robust]
- Incentive compatibility vs participation (individual rationality) constraints frame feasible policy. Trade-offs: extracting information rents vs efficiency (e.g., nonlinear tariffs; regulatory screening).
- Strategy-proofness impossibility: Gibbard–Satterthwaite shows no non-dictatorial, deterministic, strategy-proof voting rule with three or more options and unrestricted preferences. Implication: expect manipulation; constrain domains (single-peaked) or accept randomness. [robust]
Default: design for robustness—simple rules that work tolerably well across type distributions; avoid mechanisms that hinge on fine-tuned transfers or beliefs you cannot enforce.

Dynamic politics and repeated interaction. Many relations are repeated; reputations and trigger strategies sustain cooperation.
- Folk theorems: with high enough patience, many outcomes are sustainable. This is too permissive; constrain with noise, monitoring limits, and finite horizons. [mixed]
- Term limits, election cycles, and discounting shape incentives. Prediction: myopia near elections; reforms early in terms; rollbacks late. Test with event timing and panel data.

Coalitions and parties. Coalition formation models predict minimal winning coalitions (Riker’s size principle) and portfolio allocation proportional to seat shares (Gamson’s law). Evidence: Gamson’s law is approximate; surplus coalitions occur under high uncertainty or external threats; formateur advantage is real. [mixed] Default: treat coalition size as balancing office rents vs policy distance and survival risk; incorporate informateur/formateur institutions and president vs parliament roles.

Link to policy analysis. Optimal policy in models is rarely feasible administratively. Bring in bureaucratic capacity: add implementation noise; require simplicity. Prediction: second-best rules (linear taxes, simple quotas) outperform complex first-best designs in low-capacity settings. [robust] Use this to justify default policy heuristics (e.g., broad bases, low rates in taxation).

Quick modeling checklist:
- What is the strategic tension? (cooperation, coordination, screening, moral hazard)
- What is known by whom and when?
- What credible actions can be taken? Are threats enforceable?
- What outside options exist?
- Which equilibrium is focal, and why in this context?
- What comparative statics matter for your question?
- What empirical traces would confirm each implication?

Practice exercises:
- Write a two-player signaling model of candidate valence with a costly advertising signal. Derive separating and pooling conditions; predict how campaign finance limits change equilibria.
- Build a Baron–Ferejohn miniature with three legislators. Compute expected payoffs for proposer and others as discounting varies; predict how stricter amendment rules shift shares.

Normative frameworks guide what counts as a good political order and policy. Use them to make your value claims explicit and testable in argument.

Authority and political obligation. Why obey? Main lines:
- Consent: actual consent (rare), tacit consent (residence, using roads), hypothetical consent (what rational agents would accept). [contested] Default: use hypothetical consent as a heuristic (veil-of-ignorance style), but don’t pretend it binds those who reject it.
- Fair play: if you benefit from a cooperative scheme, you owe your share. Works best where exclusion is hard and benefits are real (public goods). [mixed]
- Natural duty: a duty to support just institutions, not necessarily particular states. [contested]
- Democratic authority: legitimacy from equal say, either intrinsically (respect) or instrumentally (better decisions). [mixed]
Quick check: if participation is meaningful, benefits are non-excludable, and exit is costly, fairness-based obligations are strongest; where institutions are unjust or exclusion is feasible, obligations weaken.

Justice and distribution. Competing standards:
- Utilitarianism (maximize aggregate welfare). Strength: comparability and clear trade-offs. Weakness: can sacrifice rights and fairness. [contested]
- Prioritarianism (give extra weight to the worse-off). Captures moral concern without strict equality. [mixed]
- Rawlsian justice (equal basic liberties; fair equality of opportunity; difference principle: maximize the minimum). Strength: protects rights and structure. Critiques: idealized, vague on feasibility. [mixed]
- Libertarian entitlements (justice in acquisition, transfer, rectification; minimal state). Strength: respects voluntary exchange; critiques: ignores structural power and starting positions. [contested]
- Equality of resources vs capabilities (Sen/Nussbaum). Capabilities stress real freedoms to do and be; resource views stress fair shares given tastes and responsibility. [mixed]
Default for policy triage: rights constraints first (no torture), then maximize a weighted social objective that gives priority to the worst-off, subject to feasibility and incentive constraints.

Freedom. Three rival notions:
- Non-interference (negative liberty): fewer external constraints. [mixed]
- Self-mastery or positive liberty: autonomy through self-rule; risks paternalism. [contested]
- Non-domination (no arbitrary power over you). Good for evaluating institutions and private power (employers, platforms). [mixed]
Capabilities fuse freedom with functionings; use when outcomes and opportunities diverge. [mixed]

Democracy: value and design.
- Intrinsic: equality and autonomy demand equal political power. [mixed]
- Instrumental: better policy via information aggregation and accountability (Condorcet logic needs independence and competence). [mixed]
- Epistocracy (rule by the knowledgeable) tempted by competence; opposed on inclusion and abuse concerns. [contested]
- Deliberative democracy (public reason, justification citizens can accept). Strength: legitimacy through transparency and respect; weaknesses: exclusionary discourse, time costs. [mixed]
Defaults: pair electoral representation with participatory and deliberative supplements (citizens’ assemblies on complex issues); guard against domination by ensuring agenda access and transparency.

Pluralism, identity, and recognition.
- Multiculturalism: group-differentiated rights to secure equal citizenship vs uniform rules to avoid segmentation. [mixed]
- Feminist and critical race theories highlight structural oppression, care ethics, and intersectionality; push beyond formal equality to lived equality. [mixed]
- Indigenous and decolonial thought stress sovereignty, land, and epistemic justice. [contested in practice, often robust in historical diagnosis]
Default: evaluate policies for disparate impact, power asymmetries, and voice; add recognition and accommodation when uniform rules encode disadvantage.

Global justice. Are duties global or state-bound?
- Cosmopolitans: equal moral worth across borders; support freer movement, global redistribution, climate justice. [contested]
- Statists/associativists: special duties to co-nationals; global duties are weaker. [mixed]
- Migration: open borders increase global welfare but strain local solidarity; rights vs capacity trade-off. [contested]
- Climate: common but differentiated responsibilities; intergenerational justice; precaution under deep uncertainty. [robust need for mitigation; burden-sharing contested]
Default: for transboundary harms, assign duties by capacity and contribution; prioritize poorest and most vulnerable; use border measures only with clear harm and narrow tailoring.

Political violence and resistance.
- Just war: jus ad bellum (just cause, right intention, last resort, proportionality, reasonable chance, competent authority) and jus in bello (discrimination, proportionality). [mixed; core rules robust]
- Civil disobedience: public, conscientious lawbreaking to change unjust laws; accepts penalty to show respect for law overall (Rawls) vs broader resistance under systemic injustice (King). [mixed]
- Revolution: legitimacy rises with severe, persistent injustice and lack of peaceful avenues; but risks worse outcomes. [contested]
Default: protect noncombatants; pass proportionality and necessity tests; escalate only when peaceful channels are genuinely blocked.

How to argue well (and check yourself):
- Map claims: premises → conclusion; note where empirical claims enter.
- Test universality: would you accept this rule if roles reversed?
- Publicity test: can you justify your policy to those burdened using reasons they can reasonably accept?
- Feasibility and second-best: can institutions implement your rule without perverse incentives? If not, revise.
- Moral uncertainty: when torn between theories, pick options that are non-dominated across them; preserve options and reversibility.

Policy evaluation workflow (normative):
- Define options and status quo; list rights that constrain action.
- Identify stakeholders; include future generations and noncitizens if affected.
- Describe benefit and burden distribution; note power asymmetries and historical injustices that change what’s “owed.”
- Evaluate using your default rule (rights → priority to worst-off → efficiency), with explicit weights and thresholds.
- Stress-test with thought experiments (veil of ignorance; role-reversal).
- Check feasibility: capacity, corruption risk, compliance; prefer simple, transparent rules if capacity is low.
- Write the decision rule and trade-offs in one paragraph; state what evidence would shift you.

Applied illustrations (sketches):
- Pandemic lockdowns: rights to movement vs harm prevention. Temporarily restrict with conditions: necessity, proportionality, minimal infringement, time limits, oversight, and compensation for those burdened (small businesses, workers). Prioritize reopening where benefits to the vulnerable are highest (schools) and harms are large if closed. [mixed]
- Taxation: adopt broad base, progressive rates, and transfers that raise the minimum (negative income tax/EITC). Respect horizontal equity; use land value taxation to avoid distortions and address unearned rents. Guard against tax schemes that entrench domination (opaque loopholes). [robust economic design; normative rationale mixed]
- Speech and misinformation: default to content-neutral protections; regulate incitement and targeted harassment; design platform rules to reduce amplification of harm while preserving viewpoint diversity. Use transparency, due process, and user control, not opaque centralization. [contested]

Pitfalls in normative analysis:
- Status quo bias disguised as realism; test against reversible thought experiments.
- “Consent” without exit or alternatives; treat coerced or structural consent skeptically.
- Treating preferences as sacrosanct when shaped by domination; but avoid paternalism creep—ask for independent signs of harm and lack of alternatives.
- Ideal theory that ignores transition costs and political backlash; plan non-ideal, sequenced reforms.

Exercises:
- Write a one-page brief on cash bail reform. Apply the workflow; state your rights constraints, priority rule, and feasibility adjustments. Anticipate one strong counterargument and answer it.
- Evaluate a carbon tax with border adjustments vs regulation-only approach. Compare distributive and rights impacts domestically and globally; specify compensation for losers.
- Design a citizens’ assembly to allocate pandemic relief: selection, agenda, expert input, decision rule; justify on legitimacy and efficacy grounds.

Transition to practice. Normative clarity improves positive research: it sharpens which outcomes matter, whose welfare counts, and what trade-offs are acceptable. Use it to motivate measurement and to interpret “significant” effects with moral weight, not just statistical noise.

Comparative politics now asks why political regimes, institutions, and policies vary across countries and time, and how they change. We start with states, regimes, and their origins; then move to authoritarian politics, democratization, parties, and policy variation.

